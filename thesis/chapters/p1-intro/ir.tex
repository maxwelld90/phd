%!TEX TS-program = xelatex
%!TEX root = ../../maxwell2018thesis.tex

\chapter[Information Retrieval]{Information Retrieval:\\A History and Background}\label{chap:ir_background}

Searching for information on computers is today commonplace, thanks to the proliferation of the~\gls{acr:www} and commercial search engines\footnote{Or, as we refer to them in this thesis, \emph{retrieval systems.}}. Despite potential negatives that these technologies may bring -- turning us into \emph{shallow thinkers}~\citep{carr2008google_stupid}, for example -- retrieval systems today by and large make our lives easier, allowing us to find the proverbial needle in the haystack with minimal effort. These results are returned to us while honouring the implicit searcher contract of a timely response (in the order of milliseconds).

\begin{figure}[h]
    \centering
    \vspace{4mm}
    \resizebox{1\hsize}{!}{
    \includegraphics{figures/ch2-searchbox.pdf}}
    \label{fig:searchbox}
    \vspace{-5mm}
\end{figure}

Central to the development of retrieval systems is the study of~\glsfirst{acr:ir}. One of the key developments in the advancement of~\gls{acr:ir} as a field was the creation of a \emph{de facto} approach to studying~\gls{acr:ir} and performing~\gls{acr:ir} experimentation. This was developed in tandem with a series of \emph{retrieval models} underpinned by different theories, and the means by which we could \emph{evaluate} their effectiveness. We begin this chapter with a brief overview of the history of~\gls{acr:ir}, acknowledging the manual and mechanised systems that predate contemporary computer-based retrieval systems. After this, we move on to discussing the basics of what constitutes an~\gls{acr:ir} system. From there, we discuss work that has switched the core focus of research from the \emph{system} to the \emph{searcher,} introducing the field of~\glsfirst{acr:iir}. Included in our discussion of the searcher are some of the current \emph{searcher models} that encapsulate the different activities that they may perform. We then conclude the chapter with a discussion of the various measures used for~\gls{acr:ir} and~\gls{acr:iir} evaluation.

\section{A (Brief) History of Information Retrieval}\label{sec:ir_background:history}
While many associate the study of~\gls{acr:ir} with computers, the need to seek information in a quick and effective manner has existed throughout human history. In this section, we provide a very brief overview of some of the key advancements in what can be considered to be the study of~\gls{acr:ir} -- from library cataloguing approaches, to contemporary retrieval systems.\footnote{An excellent, in-depth discussion on the history of~\gls{acr:ir} is provided by~\cite{sanderson2012history_of_ir}.}

 %This ranges from library indexing systems, to more contemporary retrieval systems that deal with the enormous volumes of content available on the~\gls{acr:www}.

\vspace*{-2mm}
\subsection{Libraries and Mechanisation}
\vspace*{-2mm}
Containing a large volume of books discussing a virtually unlimited range of categories, \emph{libraries} require the need for a means of organising (and thus easily locating) information with relative ease. \emph{Catalogues} provide a way in which to achieve this, with ancient Greek poet Callimachus being the first person to create a catalogue in the third century BC~\citep{eliot2009companion}. A more recognisable approach to categorising content was devised by~\cite{dewey1891dcs} with the \emph{Dewey Decimal System}. The use of cards as an \emph{indexing system} was also considered by individuals such as~\cite{soper1920patent} who invented a system of providing information on what category a card belonged to based upon a punched hole.

However, finding information using these techniques was \emph{slow.} In order to speed up the process of finding useful material, mechanised techniques were also used. Allowing for searching at the rate of $600$ cards per minute, Luhn devised in the early 1950s a mechanised system that utilised punchcards and light. As stated by~\cite{sanderson2012history_of_ir}, this was also around the time that the term~\glsfirst{acr:ir} was used~\citep{mooers1950theory}. From this point in history, computer technology was developing at a rapid rate. Ultimately, this led to computerised systems superseding mechanised approaches~\citep{jahoda1961electronic_searching}.

\subsection{The Rise of Computers}
Computers now provide the underlying technologies with which we closely associate with a typical, contemporary~\gls{acr:ir} system.~\cite{sanderson2012history_of_ir} state that digital storage capacity (e.g. hard disks, and more recently, solid state storage) roughly doubles every two years. This claim is essentially analogous to the famous \emph{Moore's Law}~\citep{moore1965law}, which observes that the number of transistors in a processor (or other integrated circuits) doubles roughly every two years.\footnote{As of 2019, it is becoming increasingly difficult to develop integrated circuits that meet this rule of thumb.} Indeed, the speed at which modern day computers can search vast indexes and databases of content is vastly superior to traditional cataloguing approaches. These technological advances permit the near instantaneous returning of results from an initial request, with searchers expecting a set of results in the order of milliseconds.

Progressing from computers was the development of \emph{computer networks,} permitting the transmission of information between computers over increasingly large geographical distances. With the development of the Internet, the scene was set for the introduction of one key technology -- the~\glsfirst{acr:www}.

\subsection{The World Wide Web}
The distribution and ability to search for information over computer networks such as the Internet was traditionally undertaken with legacy protocols such as \emph{Gopher.} Gopher would provide a series of options for a user to select (i.e. categorisation of content), akin to the traditional library cataloguing approaches described above.

\begin{figure}[t!]
    \centering
    \resizebox{1\hsize}{!}{
    \includegraphics{figures/ch2-yahoo.png}}
    \caption[Screenshot of \emph{Yahoo!} Search, July 1998]{A screenshot of the landing page of \emph{Yahoo!}, as shown on July 5\textsuperscript{th}, 1998. Notice the link for the 1998 \emph{FIFA World Cup} that was taking place at the time. More central to this thesis is the inclusion of a list of page categories in conjunction with the now ubiquitous search box. Screenshot acquired from the \href{https://web.archive.org/web/19980705003104/http://www.yahoo.com}{\emph{Internet Archive}} (under fair use).}
    \label{fig:yahoo}
\end{figure}

The advent of the~\gls{acr:www} in the early 1990s brought about a new type of~\gls{acr:ir} system -- \emph{web retrieval systems.} Regarded as the first experimental web retrieval system, \emph{JumpStation} was outlined by~\cite{mcbryan1994taming_tools}.\footnote{With \emph{JumpStation} developed and hosted at the \emph{University of Stirling,} could one make the claim that web retrieval systems are a Scottish invention?} In this system, \emph{anchor text} within \emph{hyperlinks} of~\gls{acr:html} pages could be exploited to aid the ranking of documents. However, popular web retrieval systems of the 1990s initially followed the categorisation approach hailing back from libraries, as illustrated in Figure~\ref{fig:yahoo} with a screenshot of \emph{Yahoo!} from 1998. This categorisation approach on the Yahoo! front page ties in with the surfing paradigm described back in Chapter~\ref{chap:intro}. However, as the volume of information on the~\gls{acr:www} rapidly increased, this way of presenting information became impractical. It was not long before the now contemporary paradigm of search took hold, allowing individuals to dictate their information needs through the issuance of a query.

The processes that take place from query issuance to the returning of results can be considered as the study of contemporary~\glsfirst{acr:ir}. As we will discuss throughout the remainder of this chapter, work includes aspects such as the basic components of a retrieval system and approaches used for the evaluation of such systems.

\section{Information Retrieval Basics}
An~\gls{acr:ir} system is expected by the searchers that use it to return results that can be considered \blueboxbold{relevant} to their information need. Typically, these results should be \blueboxbold{ranked} by decreasing order of relevance. This was originally hypothesised by~\cite{luhn1957ranking_query}, and succinctly expressed by~\cite{robertson1977prp}.

\begin{quote}
    \emph{``A [reference] retrieval system should rank references in the collection in order of their probability of relevance to the request, or of usefulness to the user, or of satisfying the user.''}
    \attrib{\cite{robertson1977prp}}
\end{quote}

Such a system would search through a collection of \emph{unstructured} or \emph{semi-structured} data (such as a collection of web pages or other text documents, or even images or videos, representing \emph{multimedia retrieval}) before returning potential matches to the searcher.

\blueboxheader{Unstructured and (Semi-)Structured Data}
A key difference between a traditional database system -- or~\gls{acr:rdbms} -- and an~\gls{acr:ir} system is the type of data that they consider. While a~\gls{acr:rdbms} considers \emph{structured data,} an~\gls{acr:ir} system, in contrast, considers \emph{semi-structured data}, as illustrated in Figure~\ref{fig:structured_data}. With an~\gls{acr:ir} system, such a premise for structured data does not exist.\footnote{This may be a slight misnomer; \emph{schemas} can be used for an~\gls{acr:ir} system \emph{index} when considering \emph{fielded retrieval}. For example, a collection of newspaper articles may contain a title and body -- but within the fields, the data is unstructured to the retrieval system.} Semi-structured data such as an~\gls{acr:html} page contains a series of \emph{elements} (e.g. section headers represented within header elements such as \texttt{<h1>}, \texttt{<h2>}, \texttt{<h3>} up to \texttt{<h6>}), but the text within these elements is largely of an unstructured nature. The unstructured data can contain information such as dates or entities (terms describing a real-world object and/or location, such as \texttt{canberra} or \texttt{dropbear}, and can be -- as it is probably written in a natural language -- ambiguous. As such, examining this unstructured data presents a major challenge to researchers.

\todo{THURSDAY GOT TO HERE}

\begin{figure}[t!]
    \centering
    \resizebox{1\hsize}{!}{
    \includegraphics{figures/ch2-structured.pdf}}
    \caption[Structured and (semi-)structured data]{Examples of structured and (semi-)structured data. On the left is a structured~\gls{acr:rdbms} schema, represented in \emph{compressed Chen notation}~\citep{chen1976notation}. Different data types can be specified for each field, representing data in a structured way. On the right is an example of semi-structured data, showing a document from a newswire collection. Note the semi-structured component (containing an identifier and title), and the unstructured body text.}
    \label{fig:structured_data}
\end{figure}

Being able to effectively sift through large volumes of unstructured data led to the development of retrieval systems. Consisting of a number of key components, the basic process of a retrieval system -- along with the \emph{users,} or \emph{searchers} that utilise such systems -- can be seen in Figure~\ref{fig:irs}. Core to the wider system is the \blueboxbold{retrieval engine}, of which many \emph{experimental}\footnote{\cite{rijsbergen1979ir} defines a difference between \emph{operational} and \emph{experimental}~\gls{acr:ir} systems. A majority of individuals will only ever interact with an operational system (such as Google). The work in this thesis however focuses more on experimental~\gls{acr:ir} systems, and the methodology employed to compare different experimental retrieval systems against each other.} retrieval engines can be selected based upon experimentation requirements and existing infrastructure available. Examples include \emph{Lemur/Indri,} \emph{Lucene for~\gls{acr:ir}}, \emph{Okapi,} the \emph{Terrier~\gls{acr:ir} platform,} \emph{Wumpus} and \emph{Zettair.} Common to all systems are three key inputs, which are:

\begin{itemize}
    \item{an \blueboxbold{index} of documents, a specially crafted data structure used for the fast lookup of documents derived from a source collection, or \emph{corpus};}
    \item{a \blueboxbold{retrieval model}, that scores and identifies documents that may constitute as relevant to what is being searched for; and}
    \item{a \blueboxbold{query}, the construct that represents a given \emph{information need} by a searcher -- or one of several queries issued in a \emph{batch environment.}}
\end{itemize}

\begin{figure}[t!]
    \centering
    \resizebox{1\hsize}{!}{
    \includegraphics{figures/ch2-irs.pdf}}
    \caption[Basics of an experimental retrieval system]{The core components of a retrieval system, including the key processes that we discuss in this chapter, highlighted in blue boxes. Central to the discussion in this chapter is the delineation between \emph{system-sided} and \emph{user-sided} evaluation, with both clearly separated in this figure by the dashed line. On the top, system-sided aspects include the \emph{retrieval engine, retrieval model} and \emph{index.} Below, user-sided aspects include the \emph{interface}, \emph{interactions} that take place with said interface, and constructs such as the searcher's \emph{information need} and derived \emph{query/queries.}}
    \label{fig:irs}
\end{figure}

The retrieval engine combines these inputs to yield an output. This is a ranked list of documents\footnote{Depending upon the retrieval model used, ranking may or may not occur -- refer to Section~\ref{sec:ir_background:basics:models} for more information.} that the retrieval model concludes to be relevant to the given query. This is often called the \emph{matching process.} The retrieval model is responsible for performing the matching of documents from an index. This index typically constitutes a number of different data structures that are generated through the \emph{indexing process}, where a source document corpus is traversed. As highlighted by the blue boxes in Figure~\ref{fig:irs}, we discuss the indexing process and various retrieval models in this chapter, explained in Sections~\ref{sec:ir_background:basics:indexing} and~\ref{sec:ir_background:basics:models} respectively. These components are all considered as \blueboxbold{system-sided} aspects of the wider retrieval system, with evaluation of system-sided aspects concerning the quality of returned rankings, how efficient the retrieval engine is, etc.

However, the system-sided aspects only tell part of the wider retrieval process. We build retrieval systems to help searchers satisfy their information need -- and hence the study of~\glsfirst{acr:iir} is devoted to considering the interactions between the searcher and retrieval system. While we discuss more \blueboxbold{user-sided} aspects later on in this chapter (Section~\ref{sec:ir_background:user}), searchers, given an information need, will issue one or more queries, and \blueboxbold{interact} with the presented interface~\citep{ingwersen2005theturn}, or~\glsfirst{acr:serp} (refer to Section~\ref{sec:ir_background:user:iir:serp}). Their ultimate goal is to satisfy their said information need. User-sided evaluation is also considered extensively in this thesis. Examples include the examination of the many different interactions that take place, and how the presentation of results affects search behaviours.

Before discussing said user-sided aspects, we turn our attention to the \emph{system-sided} components of the wider retrieval system, considering the indexing process and various retrieval models that are commonly employed.

\subsection{The Indexing Process}\label{sec:ir_background:basics:indexing}
The process of indexing takes into account the conversion of a collection of documents (or corpus) into a data structure that facilitates fast, full-text search -- a key requirement of any retrieval system. This full-text search is typically undertaken in milliseconds, with the goal of finding documents that will be relevant to a given query (and thus information need). The additional storage space and management requirements to maintain an index of documents are considered to be a necessary tradeoff to guarantee timely responses to a searcher's query.

As illustrated in Figure~\ref{fig:inverted}, the indexing process can be split into three main steps:

\begin{itemize}
    \item{gathering the corpus of documents to be indexed;}
    \item{performing pre-indexing data preparation; and}
    \item{creating the various data structures that constitute an index.}
\end{itemize}

Experimental corpora are available for use with batch experimentation, typically from various evaluation forums, as discussed in Section~\ref{sec:ir_background:paradigms:trec}. For operational retrieval systems, data is collated by other means. For example, web retrieval systems employ a \emph{web crawler} to examine pages on the~\gls{acr:www}, and accumulates additional content by following the~\glsplural{acr:www} hyperlink structure. Google's crawler, \emph{Googlebot,} regularly crawls high impact websites to ensure that the associated index is continually refreshed with up to date information.

\begin{figure}[t!]
    \centering
    \resizebox{1\hsize}{!}{
    \includegraphics{figures/ch2-inverted.pdf}}
    \caption[Illustration of an inverted index]{A demonstration of an \emph{inverted index}, with three source documents for comparison. Depending upon the requirements of the~\gls{acr:ir} system, the indexing process may vary; all classical~\gls{acr:ir} systems however rely upon some form of inverted index.}
    \label{fig:inverted}
\end{figure}

An index will contain an entry for each processed document, along with a \emph{vector of terms} that are present within said document. This is known as the \emph{forward index.} However, a retrieval system needs to support fast full-text search, matching terms from a searcher's query to one or more documents within the index. To support faster query matching, the most simplistic approach is to simply \emph{invert} the index, such that the lookup of the index then corresponds to individual terms, not individual documents. A \emph{vector of documents} can then be provided for each term, yielding much faster access to a potential list of documents. An example of an inverted index is provided in Figure~\ref{fig:inverted}. The source corpus in this example illustration consists of three documents, and the resultant inverted index is shown. The set of documents retrieved can then be sent to a retrieval model for ranking.

Before a document is indexed, a number of pre-indexing steps usually take place. Three of the most common processes involved within such a pipeline include \emph{tokenisation,} \emph{stopword removal} and \emph{stemming.}

\subsubsection{Tokenisation}
Put simply, tokenisation is the process of \emph{parsing} a source document and splitting the data within the document into a number of individual \emph{tokens} that may be subsequently indexed. A token is considered as a sequence of characters, grouped together to be semantically useful for processing a given document. While we do not go into greater detail about the process of tokenisation, there are many challenges to this process -- such as \emph{word boundary ambiguity}. While parsing an English or Latin-based document may be relatively straightforward (with spaces representing \emph{word boundaries}), other language structures (such as Chinese, Japanese, Thai or Korean) could present an issue. Considering what words a potential searcher of a retrieval system may use to search with may be a potential pathway for finding a solution to this problem.

\subsubsection{Stopword Removal}
Stopword removal is another popular choice for indexing document collections for use in an experimental~\gls{acr:ir} system. Illustrated in Figure~\ref{fig:inverted}, extremely common words which would appear to have little value in selecting documents matching a searcher's query (that is, \emph{non-discriminative} words) can simply be removed from a document's vocabulary entirely. Examples of such \emph{stopwords} could be \emph{the}, \emph{a}, or \emph{did}, or even a complete phrase from the famous soliloquy of William Shakespeare's \emph{Hamlet:} \emph{``to be or not to be''.} Some experiments consider a small list of stopwords, while others consider a larger list, with larger lists often significantly reducing the size of an indexed corpus~\citep{manning2008ir}. Indeed, it was argued by~\cite{fox1992stopwords} that larger lists \emph{``are advisable''}.

While stopword lists may be manually crafted under particular scenarios, automatic extraction from a document corpus is perhaps a more common practice. A simple approach would be to count the \emph{term frequency} for each term within a corpus and sort the resultant list in descending order, selecting some top \emph{k} of the most frequently occurring terms. Readily available lists are also available.~\cite{rijsbergen1979ir} for example produced a list of 250 terms, with~\cite{francis1985stopwords} demonstrating a list of 425 stopwords from the \emph{Brown corpus}\footnote{The \emph{Brown corpus} was a collection of documents representing (then) contemporary American English, compiled by William Francis and Henry Ku\v{c}era -- refer to~\cite{francis1979brown_manual} for more information.}. For the experiments detailed in this thesis, \emph{Fox's classical stopword list}~\citep{fox1992stopwords} is used, consisting of 421 terms. Such an approach may be considered fine, but stopwords lists do vary from collection to collection, as stated by~\cite{lo2005automatically}.

Issues of course also exist with the removal of stopwords. Removing stopwords from a query may decrease processing time, but what if all terms within a query are stopwords, like the aforementioned soliloquy? The resultant query passed to the retrieval engine could contain zero terms! As such, commercial retrieval systems are \emph{less likely} to employ stopword removal to counter such an occurrence~\citep{manning2008ir, dolamic2010stopword}. Techniques such as compression may be employed to keep the size of the resultant index down. Queries such as \texttt{`to be or not to be'} may well contain some semantic meaning. Like tokenisation, there is often more to this problem than initially meets the eye.

\subsubsection{Stemming}
Another common pre-indexing process is \emph{stemming} (also called \emph{lemmatisation).} This is the process of reducing inflected -- or sometimes derived -- words from their \emph{word stem, base} or \emph{root.} For example, given the terms \texttt{fisher}, \texttt{fished} and \texttt{fishing}, reducing each of these terms to their respective word stem would result in \texttt{fish}. Essentially, stemming allows one to group words together with a similar, basic semantic meaning. This provides the advantage of reducing the size of an index, with fewer terms to index. A further benefit may be the potential increase in the number of possible matches that can be found with a stemmed set of query terms, increasing the retrieval system's \emph{recall} (refer to Section~\ref{sec:ir_background:evaluation:system:recall}).

The concept of stemming has been studied since the 1960s, with the \emph{Porter stemmer}~\citep{porter1980algorithm} emerging over time as empirically the most effective.\footnote{The Porter stemming algorithm is not provided in this thesis; refer to~\cite{porter1980algorithm} for an in-depth explanation of the algorithm.} Comprised of a series of linguistic rules, the \emph{measure} of a word can be considered; \emph{``loosely checking the number of syllables to see whether a word is long enough that it is reasonable to regard the matching portion of a rule as a suffix rather than as part of the stem of the word.''}~\citep{manning2008ir}. Porter stemming is utilised in the indexing process for the work reported in this thesis; other stemmers do exist, with examples including the original single pass stemmer devised by~\cite{lovins1968development}, and the Krovetz stemmer~\citep{krovetz1993stemming}.

Issues such as \emph{overstemming} can impact upon a retrieval system's performance. Here, terms are reduced so far back to the point that meaning is lost, thus negatively affect the results returned. Terms like \texttt{universe}, \texttt{university} and \texttt{universal} when stemmed will be reduced to \texttt{univers}. While the three original terms may be etymologically linked, their modern meanings are however very different. When stemming is applied, documents containing both \texttt{universe} and \texttt{university} will be returned. While we do not go into depth into the solutions to this problem, one potential workaround considers the \emph{$n$-gram} context of a term, allowing the retrieval system to thus select the correct meaning~\citep{mcnamee2005stemminggrams}.

\subsection{Retrieval Models}\label{sec:ir_background:basics:models}
Given a generated document index and a searcher's query, the next part of the process is retrieval (or \emph{matching}). For this, a number of mathematically-based \emph{retrieval models} have over the years been developed that attempt to operationalise the notion of relevance. Such models provide us with a means for discussion and further refinement. They also provide us with the blueprint from which we operationalise a retrieval system~\citep{hiemstra2009ir_models}. The usefulness of such a model can be subsequently tested via experimentation and evaluation.

Several different types of retrieval model have been defined, ranging from the relatively simplistic, to the more complex. More complex approaches not only define a notion of what documents would be considered relevant but also to what \emph{degree} that is so. This section considers three main retrieval model families, including:

\begin{itemize}
    \item{the \emph{boolean model;}}
    \item{\emph{probabilistic models;} and}
    \item{the \emph{vector space model.}}
\end{itemize}

This summary is not exhaustive. A different approach could be a \emph{language model} that considers a probability distribution over a sequence of words (and thus is probabilistic)~\citep{manning2008ir}. A more contemporary ranking approach would be \emph{neural~\gls{acr:ir} models} that use neural networks to rank documents in relation to a searcher's query~\citep{mitra2017neural_ir}. These are not discussed here -- this section provides a broad overview of approaches used in this thesis, discussing the benefits and disadvantages of each approach. We focus the discussion of each retrieval model emphasising how it can potentially influence \emph{stopping behaviours.}

\subsubsection{Boolean Model}
Cited as the first formally defined~\gls{acr:ir} retrieval model, the boolean model is also the most likely one to be criticised~\citep{hiemstra2009ir_models}. The model employs operators of mathematical logic as defined by George Boole~\citep{boole1847mathematical}, or \emph{set theory.} Boole defined three basic operators: \texttt{AND}, yielding a logical product between two sets; \texttt{OR}, yielding the logical sum between two sets; and \texttt{NOT}, yielding the logical difference.

\begin{figure}[t!]
    \centering
    \resizebox{1\hsize}{!}{
    \includegraphics{figures/ch2-boolean.pdf}}
    \caption[Venn diagrams illustrating boolean retrieval]{An example illustration of the boolean retrieval model, using the query terms \texttt{glasgow}, \texttt{university} and \texttt{computing}. Each disc represents the set of documents containing that particular term. In the figure, three Venn diagram examples are provided, demonstrating the key logical operators used (\texttt{AND}, \texttt{OR} and \texttt{NOT}). Areas in blue are returned in the example boolean query provided underneath each Venn diagram.}
    \label{fig:boolean}
\end{figure}

By considering an individual query term and an unambiguous set of documents, logical operations can be applied to retrieve a set of documents. For example, the query term \texttt{glasgow} will yield a set of all documents containing the term \texttt{glasgow}, yet the query \texttt{NOT glasgow} will retrieve the set of documents that \emph{do not} contain any mention of the term \texttt{glasgow}. These results of applying logical operators between different sets can be illustrated through a \emph{Venn diagram,} where each set of documents is represented as a disc. Figure~\ref{fig:boolean} provides an example of such diagrams, using \texttt{glasgow university computing} as an example.

Despite its relative simplicity, there are major limitations to the exact match approach. First, when considering the boolean query, there is no notion of term importance -- every term has equal weighting. Issuing a query utilising rules of logic also appears as a relatively unnatural representation of the information need of a searcher. Indeed, as an information need becomes more complex, the corresponding boolean query can grow to be disproportionately large and cumbersome. As documents either belong to a set or not, a document is considered to be either relevant \texttt{(TRUE)} or not \texttt{(FALSE)}. As such, one cannot estimate the degree as to how relevant a document may be to the searcher's query, and thus results are provided to the searcher in an unranked manner.

Returning an unranked set of documents would appear as an alien concept to users of contemporary retrieval systems -- one would assume that the document presented first would be the document considered to have the greatest relevance, as per the underlying retrieval model. This would make it difficult for a searcher to obtain some notion of how many results he or she should examine before stopping. This is because no ranking means all returned documents are of equal importance. 

Instead, a searcher utilising a boolean retrieval system will often find an initial exploratory query will return a large set of documents, too many to examine each in sufficient detail. Rather, what a searcher will do is gradually reformulate their query in an iterative manner~\citep{koch2009iterative} -- like in the illustration below -- until the document set returned is of a manageable size to process. This is an inherently different kind of stopping behaviour from the examples provided thus far in this thesis which assumes documents are presented in a \emph{ranked} list, with some notion of a \emph{depth} at which a searcher would stop.

\begin{figure}[h]
    \centering
    \vspace{4mm}
    \resizebox{1\hsize}{!}{
    \includegraphics{figures/ch2-boolean-reduce.pdf}}
    \label{fig:boolean_reduce}
    \vspace{-5mm}
\end{figure}

Despite not being required in contemporary retrieval systems, many systems still do provide support for crafting a boolean query for when returning a good set of results is difficult. Boolean queries may also be of use where ambiguity exists within a searcher's query, and clarification is required to eliminate a set of non-relevant documents. Indeed, boolean queries still find considerable traction in professional search systems, such as patent search. Here, missing an existing, relevant patent may be incredibly costly -- here, \emph{recall} is preferred over \emph{precision}, as discussed in Sections~\ref{sec:ir_background:evaluation:system:recall} and~\ref{sec:ir_background:evaluation:system:precision} respectively.

\subsubsection{Probabilistic Models}\label{sec:ir_background:basics:models:probabilistic}
The next major development was probabilistic retrieval models, defined to estimate the likelihood of a document being relevant to a given query. One of the most well-known ranking principles, known as the~\gls{acr:prp} -- defined by~\cite{robertson1977prp} -- who in turn attributed the development to~\cite{cooper1971relevance} -- states the following:

\begin{quote}
\emph{``If a reference retrieval system's response to each request is ranking of the documents in the collections in order of decreasing probability of usefulness to the user who submitted the request, where the probabilities are estimated as accurately as possible on the basis of whatever data has been made available to the system for this purpose, then the overall effectiveness of the system to its users will be the best that is obtainable on the basis of that data.''}
\attrib{\cite{robertson1977prp}}
\end{quote}

Essentially, this states that documents that are considered more likely to be relevant than non-relevant should be retrieved -- or where $P(R|D) < P(\overline{R}|D)$. However, while the~\gls{acr:prp} lays much of the foundation from which probabilistic models have been derived, it does not provide its own concrete implementation of such a model.

Perhaps one of the most widely used derived models is \emph{Okapi BM25}~\citep{robertson1995trec3}. Indeed, this retrieval model has had considerable impact upon the~\gls{acr:ir} community, and is still used extensively today. BM25 provides a solid baseline for contemporary research and is the retrieval model employed in the experimentation discussed in this thesis, primarily selected for its effectiveness and popularity. Such a model, like the vector space model, again provides a ranking for documents returned -- and thus provides searchers with a gauge as to how relevant a document can be, given its ranking. Thus, it intrinsically provides a cue as to the depth at which a searcher should stop.

\subsubsection{Vector Space Model}
In conjunction with probabilistic retrieval models, further families of retrieval model were developed to counter the issues and criticisms of boolean retrieval.~\cite{luhn1957ranking_query} hypothesised that a searcher, when wishing to search for documents addressing their information need, should prepare a document that is similar to the documents being sought after. By comparing documents against this \emph{representative} document, a retrieval system could then begin to deduce what other documents would be useful, and by \emph{what margin.}

The vector space model proposed by~\cite{salton1975vsm} incorporates the principles as outlined by~\cite{luhn1957ranking_query}. These basic principles are operationalised by representing queries and documents within Euclidean geometry, where both are represented as vectors in multi-dimensional space. The notion of how close documents appear to each other denotes the relevance of a document.

The vector space model has been very popular, as it provides an intuitive means for addressing the overarching problem of a retrieval system. It can also incorporate methods such as \emph{term weighting,} which has been shown to improve retrieval effectiveness~\citep{croft2010search}. Furthermore, as queries and terms are represented in Euclidean space, vector similarity methods can be employed to determine relevance. While many approaches have been trialled, empirical evidence has favoured \emph{cosine similarity}~\citep{croft2010search}. This is illustrated in Figure~\ref{fig:vector_space}. Using such an approach allows one to then compute the degrees of relevance, meaning that matched documents can be returned in a ranked order. Provision of a ranking then gives cues to searchers interacting with the results list to form an idea of the depth at which examination should stop. However, at what threshold should a searcher stop? The aforementioned~\gls{acr:prp} suggests that such a threshold does exist. If the probability of a document being relevant is greater than the probability of it being non-relevant, a searcher should look at it. Once the probability of a document being non-relevant outweighs the probability of it being relevant, the~\gls{acr:prp} indicates that a searcher shouldn't invest time examining it. What value this threshold should be is open to interpretation, and will vary from searcher to searcher.

\begin{wrapfigure}[13]{r}{0.45\textwidth}
    \begin{center}
    \vspace*{-10mm}
    \includegraphics[width=1\textwidth]{figures/ch2-vector.pdf}
    \end{center}
    \vspace*{-4mm}
    \caption[The vector space model (cosine similarity)]{An illustration of the vector space model in Euclidean space, with each term representing a dimension. Here, the cosine similarity between query \emph{q} and document \emph{d} is shown.}
    \label{fig:vector_space}
\end{wrapfigure}

In order to understand the basic workings of the vector space approach, let us consider a query, $Q$, with each of its constituent terms placed within a term vector in $t$-dimensional space, leading to $Q = (q_1, q_2, q_3,\dotsc, q_{it})$. Consider also a document, $D_i$, with terms from the document again represented in $t$-dimensional space, yielding $D_i = (d_{i1}, d_{i2}, d_{i3},\dotsc, d_{it})$. From this notation, $d_{ij}$ represents the \emph{term frequency (TF)} of term $j$ appearing in document $i$. With each term represented as a separate dimension within Euclidean space, a weighting scheme can be subsequently applied to emphasise or understate more discriminative or less discriminative terms respectively. By applying weighting schemes, the vector space model ranks documents which promote terms that are more discriminative, thus improving the quality of the returned ranked list.

Term frequency is one of many different term weighting schemes that have been trialled over the years in~\gls{acr:ir} research. Perhaps one of the best and widely used schemes is \emph{inverse document frequency (IDF)}, proposed by~\cite{sparck1972statistical}. Here, the frequency of a term is normalised against the length of a given document. In the words of its creator, IDF allows for one to define the specificity of a term as \emph{``an inverse function of the number of documents in which it occurs.''}~\citep{sparck1972statistical}. This is useful, as non-discriminative terms that occur frequently within an index (e.g. \texttt{the}) would have a small weighting applied, with the inverse happening for more discriminative terms, better able to return a given document.

TF and IDF are typically combined together as a measure of both term appearance and importance, under an approach called \emph{TF-IDF}. For a given term $k$, one can calculate a TF-IDF score with the following equation:

\begin{equation}
tf_{i,k} \cdot idf_{k} = \frac{f_{i,k}}{\sum_{j=1}^{t} f_{i,j}} \cdot log \frac{N}{n_k}.
\end{equation}

Above, $f_{i,k}$ is the frequency of term $k$, $N$ is the number of documents in the collection used, and $n_k$ is the number of documents in which term $k$ appears at least once.

% \subsubsection{Language Models}
% The final retrieval model family that we discuss are language models, closely related to the probabilistic model we define above. Indeed, while such probabilistic models have been demonstrated to perform well empirically, adapting the~\gls{acr:prp} and subsequent developments to more advanced approaches has been difficult and is generally not intuitive~\citep{hiemstra2000language_modelling} -- the interpretation offered by probabilistic models may be considered loose, and is not always theoretically principled~\citep{whiting2015phd}. This has led to the development of more formalised statistical language modelling approaches, as defined in the fields of \emph{Natural Language Processing (NLP)}, for example~\citep{lavrenko2001language_models}.
%
% Given the strong rooting in the principles of language and associated fields, language models provide a solution to the so-called \emph{bag of words}~\citep{harris1954distributional} concept that a majority of preceding retrieval models subscribe to. Put simply, this concept considers each document and query as a bag of words, meaning that the ordering of terms loses any significance. While a simplifying assumption, losing the ordering of terms means that semantic meaning and grammar rules are lost -- and thus a retrieval engine employing a model using this simplifying assumption may lose key meanings, and subsequently retrieval effectiveness will suffer.
%
% Essentially, a language model is a probability distribution over strings of text. Given a string of text (i.e. a searcher's query), how likely is it that the given query appears in a given \emph{language}? Each document provides its own language, where we consider all possible phrases that the author of a document could have written when creating said document. Of course, some phrases are more likely than others. For example, the phrase \texttt{rain in glasgow} is more likely to appear in a document (in that order) than the seemingly random assortment of terms \texttt{purple monkey dishwasher}\footnote{If you ever watched \emph{The Simpsons}, you might disagree with this statement.}. Given this, and conceptualising a searcher's query in much the same fashion, we can produce a probability distribution $P(Q|D)$, concerning the probability of observing query $Q$ during some form of sampling within the language model of document $D$.
%
% The most simplistic approach to language modelling is the \emph{unigram} approach, where each term within a document and/or query are considered in isolation, which subscribes to the aforementioned simplifying bag of words concept. Essentially, such an approach provides a probability distribution over the words appearing in the language. Higher order \emph{grams} such as \emph{bi-grams} and \emph{tri-grams} begin to consider more the place in which terms appear with respect to others, and thus the semantic meaning defined by this positioning begins to be taken into account within the probability distribution. When considering a document, the more a document discusses a particular topic, the more likely one would begin to observe terms about that topic in said document. When a term is not mentioned in a document, smoothing can be applied (given the wider collection the document is part of) to avoid a zero probability when calculating the probability of terms permitting the partial matching of queries where not all terms appear within a target document.

\section{From System to Searcher}\label{sec:ir_background:user}
So far in this chapter, we have provided a background on several key developments in the field of~\gls{acr:ir}. However, these developments are focused exclusively on the \emph{system.} Indeed, as we previously discussed in this chapter, the purpose of a retrieval system is to satisfy the information needs of the \emph{user} (or searcher). Satisfying this information need is key to a successful retrieval system.

In this section, we discuss a line of research that moves from considering the system to a more extensive examination of the searcher and his or her interactions with a retrieval system. This is examined in the study of~\glsfirst{acr:iir}. However, before discussing the~\gls{acr:iir} process, we must first consider in more detail the paradigms that have been extensively used in traditional, \emph{system-sided}~\gls{acr:ir} research. These paradigms have been the underpinnings of ~\gls{acr:ir}'s scientific methodology for many decades, and can be largely considered to be na\"{i}ve of a searcher's behaviour. After discussing these paradigms, we then move onto our discussion of~\gls{acr:iir}, emphasising the \emph{spectrum} of research between the system-sided and user-sided extremes. This then leads onto the concept of \emph{searcher models} that attempt to capture the high-level, cognitive processes that searchers undertake.

\subsection{Experimental Paradigms}\label{sec:ir_background:paradigms}
The methodology behind the majority of classical~\gls{acr:ir} research has focused on the \emph{Cranfield paradigm.} Developed at Cranfield University in Bedfordshire, England, the experimental paradigm is based upon the \emph{Cranfield II} experiments~\citep{aslib1966factors}. The goal of these experiments was to create:

\begin{quote}
\emph{``a laboratory type situation where, freed as far as possible from the contamination of operational variables, the performance of index languages could be considered in isolation.''}
\attrib{\cite{cleverdon1991cranfield}}
\end{quote}

The experimental paradigm required the same set of documents, and the same set of information needs to be used for each language, and the use of common~\gls{acr:ir} measures, \emph{precision} and \emph{recall} (refer to Sections~\ref{sec:ir_background:evaluation:system:precision} and~\ref{sec:ir_background:evaluation:system:recall} respectively) to be used to measure a given retrieval system's \emph{effectiveness.} Core to these experiments was also the notion of a \blueboxbold{test collection}, itself consisting of three key components:

\begin{itemize}
    \item{the corpus (collection of documents) to be used;}
    \item{the statements of different information needs hereafter referred to as \blueboxbold{topics}; and}
    \item{a set of \blueboxbold{relevance judgements} -- a list of documents considered relevant that \emph{should} be retrieved by the retrieval system for each topic trialled.}
\end{itemize}

Given these three components, the Cranfield experiments made a number of major simplifying assumptions, as outlined by~\cite{voorhees2001iir_philosophy}. The first considers the notion of \emph{topical similarity,} by which their relevance is approximated. In short, all relevant documents are equally desirable, and the relevance of one given document is independent of the relevance of any other document. This also leads to the notion of a \emph{static information need}. Under Cranfield, there is assumed to be no change during the search process for what the searcher is searching. Additionally, the single set of relevance judgements provided as part of the test collection is to be considered to be \emph{representative of an entire population.} This means that for a given topic, every searcher will seek to find the same set of relevant documents. Finally, the list of relevant documents for a given topic is assumed to be \emph{total and complete,} i.e. all documents relevant to a topic have been identified and are listed.

\subsubsection{The Text REtrieval Conference}\label{sec:ir_background:paradigms:trec}
A number of different \emph{evaluation forums} have been derived from the Cranfield experimental paradigm, utilising many of the implicit assumptions. These forums promote the development of~\gls{acr:ir} as a field, fostering a drive to develop improvements in the various retrieval models and other retrieval system components. Examples of evaluation forums include \emph{NTCIR}~\citep{kando1999ntcir}, \emph{CLEF}~\citep{braschler2001clef} and \emph{INEX}~\citep{fuhr2006advances}. However, one of the most well-known evaluation forums is the U.S. Government funded,~\gls{acr:nist} sponsored~\glsfirst{acr:trec}~\citep{harman1993trec1}. Experimentation following the~\gls{acr:trec} approach is hereafter referred to as \blueboxbold{\gls{acr:trec}-style} in this thesis.

\gls{acr:trec} provides a platform for annual collaboration between research groups interested in different aspects of~\gls{acr:ir} research. Each year, a series of~\gls{acr:trec} \emph{tracks} are defined, with each consisting of a test collection, in turn consisting of the three components defined above. Within each track is a set of tasks. Some of the tasks, such as those belonging to the \emph{\gls{acr:trec} Interactive Track}~\citep{over2001trec}, are known as \blueboxbold{ad-hoc}. This type of task can be considered as one of the most obvious for search, where a searcher develops an information need in an ad-hoc fashion, and then issues a query to a retrieval system.

These tasks are used in conjunction with the relevance judgements provided by assessors. Assessors are usually employees of NIST~\citep{robertson2008history_ir_evaluation}, who were in turn previously employed as news analysts by various U.S. security agencies. A series of documents are extracted from the document collection using a simple query (a process called \emph{pooling}). Due to the potentially large size of document collections, pooling is an acceptable solution to reducing the number of documents to be examined. As an example, given the topic \emph{wildlife extinction,} the query \texttt{`wildlife extinction'} is issued. Documents returned are judged by assessors. For many~\gls{acr:trec} tracks, judgements are binary, with \texttt{0} denoting non-relevance, and \texttt{1} denoting relevance. Graded relevance judgements can also be used -- the initial Cranfield II experiments, for example, used a five-point relevance scale~\citep{voorhees2001iir_philosophy}. Pooling can mean that documents that are potentially relevant can be missed by assessors, and thus will receive no judgement~\citep{keenan2001effect}. This could mean that a retrieval system may return a document that a searcher judges to be relevant, yet will not have a judgement associated with it. This demonstrates a limitation in the assumptions of Cranfield.

Institutions wishing to participate in a track receive the associated test collection and tasks. They then index the corpus and run their experimental retrieval system over the provided material. Experiments are typically run over $25$-$50$ different topics~\citep{voorhees2001iir_philosophy}, with a solitary query issued for each (the topic's \emph{title}). These are typically executed in a batch environment, with a large number of results (typically $1,000$) returned from the retrieval engine. Output from the experiments is then produced in a standardised format. Results can then be used in conjunction with the judgements (termed \emph{Query RElevance Judgements,} or \blueboxbold{\glsplural{acr:qrel}}), and fed into a standardised program called \texttt{trec\_eval}\footnote{\texttt{trec\_eval} is downloadable from \url{http://trec.nist.gov/trec_eval/} \urlaccessed{2018-03-08}. Version 8.1 of the software was used for computing most of the evaluation measures reported in this thesis.} to perform an evaluation of the runs that have been undertaken. The application returns the values for a number of common system-sided evaluation measures, some of which are discussed in Section~\ref{sec:ir_background:evaluation:system}.

\subsubsection{The TREC Searcher Model}\label{sec:ir_background:paradigms:trec:model}
Given the assumptions of the Cranfield paradigm and subsequent evaluation forums such as~\gls{acr:trec}, one may be forgiven for thinking that the searcher -- the target audience of any retrieval system -- has been completely ignored from the process. While it is true that the paradigm focuses primarily on system-sided evaluation of retrieval systems, a searcher \emph{is} considered -- just in a highly abstracted form. The assumption that a searcher's information does not vary as they search is just one example of an abstraction from reality -- a searcher's information need is \emph{dynamic,} and typically evolves as they search~\citep{borlund2003iir_model}.

\begin{figure}[t!]
    \centering
    \resizebox{1\hsize}{!}{
    \includegraphics{figures/ch2-basic_model.pdf}}
    \caption[Flowchart of the abstracted~\gls{acr:trec} searcher model]{The~\gls{acr:trec} searcher model. Considering a highly abstracted searcher, a query is issued, with each individual result examined in a linear order, up to some depth \emph{k} (typically 1,000), before the searcher stops. No notion to relevance is considered; all documents up to rank \emph{k} are assumed by the searcher to be relevant to the topic provided.}
    \label{fig:trec_model}
\end{figure}

The basic searcher model employed by a~\gls{acr:trec}-style experiment is illustrated as a flowchart in Figure~\ref{fig:trec_model}. Given the batch-style nature of~\gls{acr:trec}-style experimentation, this particular searcher model is well suited to such an environment, as the simplifying assumptions and complete lack of interaction from the searcher go hand in hand with the design goals of the initial Cranfield II experiments. Searchers subscribing to this model, when provided with an information need (or topic), will issue a single query pertaining to said information need -- the topic's title -- and then examine each individual item, one by one, considering each item as relevant to said information need. This process continues until some rank $k$, at which point the searcher will cease and the search process ends. This rank $k$ is typically set to $1,000$ to provide evaluation programs such as \texttt{trec\_eval} with a large a set of rankings as possible for the various evaluation measures to be computed accurately.

With this highly abstracted searcher model being agonistic of the complex interactions that take place during search, a number of different criticisms can be made. Below, we enumerate on three primary criticisms that have been discussed in the literature.

\begin{itemize}
    \item{\blueboxbold{A Single Query} The~\gls{acr:trec} searcher model assumes that a single query is issued for a given information need. This severely limits the potential for interaction between the searcher and retrieval system -- a single query means no \emph{query reformulation} is possible, for example. In reality, searchers \emph{do} reformulate queries, issuing multiple queries during a search session~\citep{keskustalo2009querying}.}
    \item{\blueboxbold{Assuming a Fixed Depth} Searchers subscribing to this searcher model will always examine documents up to a depth of $k$. This assumes a \emph{fixed-depth stopping strategy,} where searchers are agnostic of the results as presented to them, etc. In reality, searchers adapt their stopping depths dependent upon a variety of different factors, such as the number of non-relevant items uncovered thus far~\citep{cooper1973retrieval_effectiveness_ii}.}
    \item{\blueboxbold{Everything is Inspected} The final key, limiting assumption in the~\gls{acr:trec} searcher model is that \emph{all} documents are examined and considered relevant. In reality, searchers may skim through results, or simply decide that a document does not look to be promising, and thus skip it. There is no concept of a \emph{result summary,} a shorthand overview of the contents of the document. Result summaries are typically expected to be part of the interface of a contemporary retrieval system.}
\end{itemize}

Over time, researchers have begun to examine ways in which to improve upon the basic, rigid assumptions laid out by this searcher model. For example,~\cite{smucker2012tbg} introduced \emph{time-biased gain,} where probabilities of interacting with documents were included. Work by~\cite{tran2017markov_models} considered the~\gls{acr:trec} searcher model from the standpoint of a \emph{Markov model.} As such, this representation of the search process was complemented with a series of different \emph{transition probabilities,} dictating the likelihood of a searcher switching from one \emph{state} to another.

These works can be considered as a means of including the complex interactions that take place between the searcher and retrieval system within a searcher model, or consideration of the wider~\gls{acr:iir} process.

\subsection{Interactive Information Retrieval}\label{sec:ir_background:user:iir}
The study of~\glsfirst{acr:iir} attempts to address our lack of understanding of a searcher's behaviours and interactions, and incorporate new findings into the evaluation of retrieval systems~\citep{callan2007minds}.~\gls{acr:iir} studies can include aspects from both user-sided and system-sided research. For example, one might present the results of a user study examining a particular phenomenon of a searcher's behaviour, and also provide details of a system-sided evaluation. As discussed by~\cite{kelly2009iir},~\gls{acr:iir} can trace its roots back to a variety of different disciplines, including: traditional~\gls{acr:ir} (i.e. exclusively system-sided research); library and information sciences; psychology; and~\gls{acr:hci}. Typically presented as a branch of~\gls{acr:ir} and/or~\gls{acr:hci}, arguments also exist to consider~\gls{acr:iir} as a distinct area of research~\citep{ruthven2008iir}.

\begin{quote}
\emph{``In~\gls{acr:iir}, users are typically studied along with their interactions with systems and information. While classic~\gls{acr:ir} studies abstract humans out of the evaluation model,~\gls{acr:iir} focuses on users' behaviors [sic] and experiences -- including physical, cognitive and affective -- and the interactions that occur between users and systems, and users and information. In simple terms, classic~\gls{acr:ir} evaluation asks the question, does this system retrieve relevant documents?~\gls{acr:iir} evaluation asks the question, can people use this system to retrieve relevant documents?''}
\attrib{\cite{kelly2009iir}}
\end{quote}

To address the question of whether \emph{people can use a retrieval system}, we begin by examining the wider~\gls{acr:iir} process. Figure~\ref{fig:irs} on page~\pageref{fig:irs} considered a number of user-focused aspects, as illustrated in Figure~\ref{fig:iir}. Given some phenomenon in the natural world (perhaps by observation, reading a book, or through conversation with another human), a searcher will then begin to formulate an information need. As discussed previously in this thesis, this information need can arise from a knowledge gap in the searcher's mind, an internal inconsistency in what they are experiencing, or a conflict of evidence. In an \emph{Anomalous State of Knowledge (ASK)}~\citep{belkin1980ask}, the searcher will then begin the~\gls{acr:iir} process, with the aim of satisfying their (perhaps vague) information need.

\begin{figure}[t!]
    \centering
    \resizebox{1\hsize}{!}{
    \includegraphics{figures/ch2-iir.pdf}}
    \caption[Basic components and interactions of the~\gls{acr:iir} process]{Basics of the~\gls{acr:iir} process, complete with a number of different searcher interactions that can take place (although this illustration may not be exhaustive). Forming an information need, searchers then begin the interaction process by issuing a query, before examining content presented on the~\gls{acr:serp}. Searchers at given points may decide to \emph{stop} their interactions.}
    \label{fig:iir}
\end{figure}

Upon bringing up the interface of a retrieval system, the searcher begins their so-called \blueboxbold{search session}, which begins with the formulation of the information need as a series of terms -- or query. Upon submitting this query to the underlying retrieval system, a complex series of \blueboxbold{interactions} begin to take place between the system and the searcher, and these interactions are of great importance to those studying~\gls{acr:iir}. Results will be retrieved by the underlying retrieval system and presented to the searcher in the form of a~\glsfirst{acr:serp}. While we discuss the~\gls{acr:serp} in more detail in Section~\ref{sec:ir_background:user:iir:serp}, it should be noted that a majority of the interactions that take place occur on the~\gls{acr:serp}.

However, interactions may be dependent upon the searcher's \emph{intent} -- that is, what they are aiming to achieve through the satisfaction of their information need. Three intents typically used are \emph{navigational, transactional} and \emph{informational}~\citep{jansen2008intents}. Navigational intents for example simply refer to the notion of using a retrieval system to navigate the searcher to some~\gls{acr:url}. This means that the searcher will simply need to click the link to satisfy their information need.\footnote{A searcher intent analysis by~\cite{jansen2008intents} showed that approximately 10\% of queries issued to web retrieval systems were either navigational or transactional in nature.}

Informational intents will require a greater degree of interaction with the retrieval system. Searchers, for example, will begin to examine the content on the~\gls{acr:serp}, examining individual summaries for potential relevance. At each stage, the searcher is continually learning, and thus the interaction cycle may prompt the searcher, for example, to provide a \emph{query reformulation} as they begin to develop their underlying mental model of the topic. A revised~\gls{acr:serp} may then begin to yield more promising results. As the searcher examines these updated results, he or she may find that a particular summary is deemed sufficiently \emph{attractive} to investigate further, and thus clicks on the provided link. Taking the searcher to the corresponding document, the searcher can then examine the document in more detail, and make a decision as to its relevance. If not satisfied, the searcher may navigate back to the~\gls{acr:serp}, and continue their examination of further results. At some point, the searcher will make a decision to \blueboxbold{stop} their interactions -- either within a given~\gls{acr:serp}, or the search session as a whole (both are illustrated in Figure~\ref{fig:iir} with stop signs). Stopping may occur for example if a searcher has satisfied their information need, has been frustrated with the retrieval system's inability to return relevant results~\citep{cooper1973retrieval_effectiveness_ii}, or from a variety of different factors external to the search process, such as time pressure.

While this example above may be highly abstract in nature, it clearly shows that the search process is extremely complex and \emph{inherently interactive}~\citep{ingwersen2005theturn}. Thus, work in the field of~\gls{acr:iir} provides a basis for developing more complex, \emph{realistic} models of the search process, improving the offering when compared to the~\gls{acr:trec}-style searcher model.

\subsubsection{The Search Engine Results Page}\label{sec:ir_background:user:iir:serp}
Core to the experience of a searcher when using a given retrieval system are the interactions that take place on its~\glsplural{acr:serp}. Figure~\ref{fig:serp_example} depicts an example~\gls{acr:serp} of the fictional retrieval system, \searchlogo.\footnote{As highlighted in the front matter, we utilise \searchlogo~throughout this thesis to illustrate various concepts.} The illustration highlights several key~\gls{acr:serp} components that are extensively referred to in subsequent parts of this thesis. At the top of the~\gls{acr:serp} is the \blueboxbold{query box}, which allows a searcher to enter (and reformulate) queries as and when they require.

The main body of the~\gls{acr:serp} is then divided up into the left rail and right rail. Contemporary~\glsplural{acr:serp} utilise the right rail to display additional components such as \emph{information cards} as illustrated in Figure~\ref{fig:serp_example}, with contemporary~\glsplural{acr:serp} thus becoming more and more complex in nature. We however in this thesis exclusively consider simplified~\glsplural{acr:serp} \emph{without} the right rail, or~\glsplural{acr:serp} comprised entirely of \emph{result summaries,} as shown on the left of Figure~\ref{fig:serp_example}.

These result summaries are typically displayed on a~\gls{acr:serp} as the \emph{ten blue links}~\citep{hearst2009_search}, or the first ten ranked results. The document that is judged to be most relevant -- as defined by the underlying retrieval model that ranks them -- is displayed at the top of the ranked list. These result summaries are short summaries of the corresponding document, and consist of three main components:

\begin{itemize}
    \item{a \blueboxbold{title} that represents the title, or headline, of a source document;}
    \item{one or more \blueboxbold{textual snippets}, providing a summary of the source document such that searchers can determine whether it is worth examining the document in more detail; and}
    \item{a \blueboxbold{source} for the document, typically an~\gls{acr:url} if the object is~\gls{acr:www}-based.}
\end{itemize}

\begin{figure}[t!]
    \centering
    \resizebox{1\hsize}{!}{
    \includegraphics{figures/ch2-example.pdf}}
    \caption[Example illustration of a~\gls{acr:serp} and its constituent components]{An example of a~\glsfirst{acr:serp} for the query \texttt{canberra australia}. \blueboxbold{Labels} illustrate the names of the key components of a~\gls{acr:serp}. Of particular relevance to the work in this thesis are the \emph{result summaries,} shown on the left rail.}
    \label{fig:serp_example}
\end{figure}

Snippets are of particular interest to the work in this thesis; we explore the effect of their length on stopping behaviours in Chapter~\ref{chap:snippets}. Snippets are typically presented in contemporary retrieval systems as \emph{query-biased}~\citep{tombros1998query_biased}. This means that the snippet text relates to terms that were present in the searcher's query. Figure~\ref{fig:serp_example} demonstrates this with the use of \textbf{bolded} terms in example snippet text. Section~\ref{chap:snippets:background:generating} on page~\pageref{chap:snippets:background:generating} provides more detail on how snippet text is generated for result summaries.

\subsection{The IR/IIR Spectrum}\label{sec:ir_background:user:spectrum}
In order to aptly describe where~\gls{acr:iir} fits into the system-sided and user-sided space,~\cite{kelly2009iir} provided an intuitive spectrum of work that bridges~\gls{acr:ir} and~\gls{acr:iir}. Figure~\ref{fig:spectrum} illustrates the spectrum, consisting of eight different categories of study. Moving from left to right in the illustration, categories shift from solely system-sided (\gls{acr:trec}-style) studies towards those that are more user-focused, considering a searcher's behaviours when interacting with a retrieval system. Below, we detail four key category types as outlined by~\cite{kelly2009iir} that have particular relevance to the work detailed in this thesis.

\begin{itemize}
        \item[]{\hspace{-6mm}\raisebox{-.2\height}{\includegraphics[height=5mm]{figures/ch2-point1.pdf}\hspace{1.5mm}}\blueboxbold{\gls{acr:trec}-Style Studies} With a majority of traditional~\gls{acr:ir} studies falling into this category,~\gls{acr:trec}-style studies focus upon the development and evaluation of system-sided research, such as retrieval models and indexing techniques. No real searchers are included \emph{in the loop} with this approach, although a simplistic, abstracted searcher model is encoded, as previously discussed in Section~\ref{sec:ir_background:paradigms:trec:model}. While assessors do create relevance judgements used for evaluation, they are not involved in the actual batch-style search process. As such, interaction is assumed to be very simplistic, with a single query issued, for example. This is illustrated in Figure~\ref{fig:trec_model} on page~\pageref{fig:trec_model}.}
    \item[]{\hspace{-6mm}\raisebox{-.2\height}{\includegraphics[height=5mm]{figures/ch2-point2.pdf}\hspace{2mm}}\blueboxbold{'User' Relevance Assessment Studies} The second category of study does explicitly consider a human \emph{in the loop,} but only in exceptionally limited circumstances. As the name of the category suggests, the humans that are employed for this category of study are used only for generating relevance assessments, perhaps because a specific corpus is used, and no pre-existing~\gls{acr:trec} relevance assessments are available.}
    \item[]{\hspace{-6mm}\raisebox{-.2\height}{\includegraphics[height=5mm]{figures/ch2-point5.pdf}\hspace{0.2mm}} \blueboxbold{\gls{acr:trec} Interactive Studies} Studies belonging to this category typically are used to evaluate a retrieval system and/or a feature of its user interface, where an experimental retrieval system is used. Typically, aspects such as searcher behaviours, their cognition or the information seeking context are examined. These studies usually aim to assist in aiding our understanding of the search process, and the development of more intuitive and user-friendly search interfaces. Interaction is considered -- search sessions in this category of study permit searchers to issue multiple queries through query reformulation, assess documents for relevance, and conduct a number of other interactions that studies closer to the left of Figure~\ref{fig:spectrum} simply do not cater for. These studies are therefore more \emph{realistic} and are thought to more accurately represent real-world searcher behaviours.}
    
\end{itemize}

\begin{figure}[t!]
    \centering
    \resizebox{1\hsize}{!}{
    \includegraphics{figures/ch2-spectrum.pdf}}
    \caption[The spectrum of~\gls{acr:ir} and~\gls{acr:iir} research (as per~\citep{kelly2009iir})]{The spectrum of conceptualising~\gls{acr:iir} research. Methods on the left consider a more system-focused approach, with those on the right considering a more user-focused approach. The fifth step within the spectrum considering~\gls{acr:trec} interactive studies is considered to be an \emph{"archetypical~\gls{acr:iir} study"}. Figure adapted from~\cite{kelly2009iir}, with support of the author.}
    \label{fig:spectrum}
\end{figure}

Moving to the right of the spectrum, the study of the searcher becomes ever more prominent, until we reach the following category that considers the \emph{experiences} of searchers.

\begin{itemize}
    \item[]{\hspace{-6mm}\raisebox{-.2\height}{\includegraphics[height=5mm]{figures/ch2-point8.pdf}\hspace{1mm}} \blueboxbold{Information Seeking Behaviour in Context Studies} In this final category of study, researchers consider the information needs of individuals. Researchers would typically observe how individuals conduct their searches, while at the same time collating qualitative data about their differing experiences. These data can then be used to motivate iterations of the design and presentation of search results, thus improving the overall experience for searchers.}
\end{itemize}

The studies represented by ~\cite{kelly2009iir} as category \raisebox{-.2\height}{\includegraphics[height=5mm]{figures/ch2-point5.pdf}}, called \emph{archetypical~\gls{acr:iir} studies,} are the type of study that we largely consider in this thesis. Indeed, in subsequent chapters, we consider a variety of different aspects that can influence the behaviour and performance of searchers -- particularly in relation to their stopping behaviours. This work is done in combination with a series of experiments that can be considered to belong to category \raisebox{-.2\height}{\includegraphics[height=5mm]{figures/ch2-point1.pdf}}. By \blueboxbold{grounding} these experiments with data collected from studies in category \raisebox{-.2\height}{\includegraphics[height=5mm]{figures/ch2-point5.pdf}}, we are able to instantiate more realistic, credible abstractions of the search process which consider some aspects of interaction. We achieve this by \blueboxbold{simulating} the behaviour of real-world searchers to examine what happens to searcher behaviour in different contexts.

\subsection{The Simulation of Interaction}\label{sec:ir_background:user:simulation}
Simulation is defined as the \emph{imitation of the operation of a real-world process or system over time}~\citep{banks1996discrete}. Such an approach allows one to gain insight into the functioning of some real-world phenomenon, such as the complex interactions that take place during the~\gls{acr:iir} process. \emph{Computerised simulation}~\citep{heermann1990computer_simulation} has become more commonplace today with increasing computational power allowing for the development of ever more complex and realistic simulations. Simulation permits one to solve a large number of problems without resorting to a \emph{``bag of tricks''}~\citep{fishwick1995simulation}, where special purpose (and often arcane) solutions must be used. Such an example would be a series of linear equations~\citep{fishwick1995simulation}. With this \emph{closed-form} approach, underlying assumptions can become twisted and altered, drifting the representation further away from the real-world phenomenon being considered.

Simulation avoids such issues by providing one with the freedom and flexibility to reduce the above assumptions. This permits a rapid means of exploring different scenarios, all at a low cost. Additionally, without needing to consider issues such as subject fatigue (within a user study, for example), simulation provides the capability of running experiments with reproducible results~\citep{azzopardi2010workshop}. 

Simulation has been employed extensively within classical~\gls{acr:ir} experimentation.\footnote{For an in-depth discussion of various classical~\gls{acr:ir} simulations, refer to~\cite{heine1981simulation}.}~\gls{acr:trec}-style experimentation may be considered as a form of simulation, where the simple searcher model discussed in Section~\ref{sec:ir_background:paradigms:trec:model} is used to simulate the searcher's interactions. We consider in this thesis the \emph{simulation of interaction,} where one attempts to mimic behaviours that a searcher exhibits when interacting with a retrieval system~\citep{azzopardi2010workshop}. This means that we can explore different searcher behaviours, methods and techniques to better understand how searchers do, could, or are likely to behave. However, such simulations are questionable and open to criticism if they are not properly motivated, grounded and validated. Therefore, there is a pertinent need to ensure that such simulations are credible abstractions of the search process and that they are seeded with data based on actual human interaction data (study category \raisebox{-.2\height}{\includegraphics[height=5mm]{figures/ch2-point5.pdf}}, as per Section~\ref{sec:ir_background:user:spectrum})~\citep{azzopardi2010workshop}.\footnote{An exception to this rule would be the exploration of \emph{what-if} scenarios, allowing researchers to examine \emph{what} would happen to a searcher's behaviour \emph{if} a particular scenario were to be applied. One example of such a study is that by~\cite{azzopardi2011economics}. Indeed, we employ such an approach in contributory chapters of this thesis, where we examine many \emph{what-if} scenarios.} Such grounding can, for example, permit \emph{stochastic simulations,} working on the notion of the \emph{probabilities of interaction} (e.g. the probability of clicking on an attractive result summary).

Within the wider~\gls{acr:iir} process, a number of different individual components have been examined through the use of simulation. These have often been independently analysed from one another~\citep{azzopardi2010workshop}, with examples of different components and associated studies listed below.

\begin{itemize}
    \item{\blueboxbold{Query Formulation and Suggestions} This component considers querying, including the generation of pseudo-realistic queries and the development of realistic \emph{querying strategies,} grounded upon the querying behaviours of real-world subjects. Examples include studies by:~\cite{azzopardi2009query_side, azzopardi2007languages, carterette2015test_collections, jordan2006cqg, keskustalo2009querying, hagen2015query_generation}; and~\cite{verberne2015personalised_queries}.}
    
    \item{\blueboxbold{Browsing Behaviours} This broader component considers the wider behaviours of searchers when examining content, including aspects such as click models, and different browsing strategies employed by searchers (e.g. can a searcher's behaviour be classified as \emph{fast and liberal,} or \emph{slow and neutral?}~\citep{smucker2011user_strategies}). In addition to the work by~\cite{smucker2011user_strategies}, examples include~\cite{carterette2015test_collections, chuklin2015click_models, guo2009click_chain}; and~\cite{pakkonen2015behavioural_dimensions}.}
    
    \item{\blueboxbold{The Influences of Cost and Time} This component examines how varying \emph{interaction costs} (i.e. the cost of issuing a query or examining a document, considered primarily in terms of the amount of time required) and time constraints can influence the behaviour of searchers. Examples include the economics-based approach outlined by~\cite{azzopardi2011economics}, and work by~\cite{baskaya2012simulating_sessions}.}
    
    \item{\blueboxbold{Performance over Search Sessions} This area of work considers how, when different aspects are changed, the performance of a searcher over a search session varies. Examples include work by~\cite{luo2014winwin} and~\cite{luo2015pomdp}.}
\end{itemize}

Of particular relevance to this thesis is the work that has been undertaken to examine a searcher's \blueboxbold{stopping behaviour}, with examples including:~\cite{carterette2015test_collections, maxwell2015initial_stopping, maxwell2015stopping_strategies}; and~\cite{thomas2014modelling_behaviour}. In these works, different \emph{stopping strategies} and \emph{stopping models} are proposed. These are considered in depth later in Chapter~\ref{chap:stopping_background}.

Simulation provides a means for examining the aforementioned components from two different standpoints:

\begin{itemize}
    \item{considering each of the individual components in isolation, as discussed above (e.g. exclusively examining querying behaviours); or}
    \item{considering the interactive search process as a whole, and attempting to capture and model an entire search session, from querying to document examination.}
\end{itemize}

The work in this thesis considers the latter, or the entire search process, meaning that potential influences of components over others can be considered. This also justifies the need for a more advanced searcher \emph{model} that captures the interactions of the wider search process. A model is a key component of a simulation or the representation of the real-world phenomenon being simulated~\citep{tocher1963art_of_simulation}. With the~\gls{acr:trec}-style searcher model outlined, we now turn our attention to considering more advanced searcher models of the~\gls{acr:iir} process.

\subsection{Searcher Models}\label{sec:ir_background:user:models}
Models of the search process attempt to capture the high level, cognitive processes that a searcher undertakes during a search session -- such as issuing a query, or examining a document for relevance. As we have discussed previously, highly abstracted searcher models have been present in system-sided~\gls{acr:ir} research, as well as implicitly encoded within a variety of different evaluation measures, as discussed in Section~\ref{sec:ir_background:evaluation}.

\cite{carterette2011effectiveness_evaluation} argues that the measures widely used within~\gls{acr:ir} research are themselves typically comprised of three distinct underlying models that are enumerated below.

\begin{itemize}
    \item{\blueboxbold{Browsing Model} Browsing models describe how a searcher interacts with a retrieval system's interface, including the~\glsplural{acr:serp} that are presented to them.}
    \item{\blueboxbold{Document Utility Model} These models encode some form of \blueboxbold{document utility} that provides a description of how utility or \emph{gain} can be derived from relevant documents that are examined; and}
    \item{\blueboxbold{Utility Accumulation Model} This final model describes how a searcher accumulates said utility over the course of an entire search session.}
\end{itemize}

Of particular interest to the work in this thesis are the browsing models that attempt to capture the broad array of interactions that take place between the searcher and the retrieval system. The~\gls{acr:trec} searcher model is a high abstracted example of such a model, yet there is a disconnect between the assumptions made in this searcher model and reality. As such, we now discuss a number of more advanced searcher models, providing better representations of the complex interactions that take place during~\gls{acr:iir}.

Seminal work undertaken by~\cite{baskaya2013behavioural_factors} presented an explicit, revised searcher model that improved upon the \emph{interactive} capabilities that could be exploited. Improvements over the~\gls{acr:trec}-style searcher model included the following:

\begin{itemize}
    \item{the ability to \blueboxbold{separately judge result summaries} presented on a~\gls{acr:serp} from the documents associated with each;}
    \item{permitting a searcher the ability to \blueboxbold{stop at a variable depth on the~\gls{acr:serp}}, avoiding fixed depth stopping behaviours as typically employed in searcher models; and}
    \item{permitting a searcher to \blueboxbold{issue multiple queries} during a search session, lifting one of the major constraints of the~\gls{acr:trec} searcher model.}
\end{itemize}

These improvements are demonstrated as a Markov model, illustrated in Figure~\ref{fig:baskaya_model}. Compared to the~\gls{acr:trec}-style searcher model illustrated in Figure~\ref{fig:trec_model} on page~\pageref{fig:trec_model}, the model can be visually seen as more complex, consisting of six main actions.

\begin{itemize}
    \item{\blueboxbold{Query Formulation} considers where a searcher formulates the terms that they wish to enter, as well as the issuance of the query to the underlying retrieval system.}
    \item{\blueboxbold{Snippet Scanning} concerns the action that considers the examination of individual result summaries for attractiveness.}
    \item{\blueboxbold{Link Clicking} occurs when a result summary is considered to be attractive enough to warrant further examination.}
    \item{\blueboxbold{Document Examination} considers the process of examining a document for relevance to the underlying information need.}
    \item{\blueboxbold{Relevance Judging} determines whether or not the document is relevant to the searcher's information need.}
    \item{\blueboxbold{Session Stopping} concerns the action that curtails the search session.}
\end{itemize}

\begin{figure}[t!]
    \centering
    \resizebox{1\hsize}{!}{
    \includegraphics{figures/ch3-baskaya.pdf}}
    \caption[Markov model of the search process by~\cite{baskaya2013behavioural_factors}]{The searcher model, as outlined by~\cite{baskaya2013behavioural_factors}. Represented as a Markov model, the model considers six steps in all. Encoded within two of the steps are decision points that a user following this model must consider in order to continue. Figure adapted from~\cite{baskaya2013behavioural_factors} with acknowledgement from the authors.}
    \label{fig:baskaya_model}
\end{figure}

These steps broadly match up the~\gls{acr:iir} process as outlined in Figure~\ref{fig:iir} on page~\pageref{fig:iir}, and described in an abstract form in Section~\ref{sec:ir_background:user:iir}.~\cite{azzopardi2011economics},~\cite{yilmaz2010browsing_utility},~\cite{carterette2011models}, and more recently~\cite{zhang2017simulation_model} have all introduced and utilised searcher models that are similar in terms of the broad set of actions as outlined by~\cite{baskaya2013behavioural_factors}. Being a Markov model, this searcher model considers each of the different actions as a \emph{state} within the model, with a series of \emph{transition probabilities} linking each of the actions together (i.e. \emph{what is the probability of a searcher moving from scanning a snippet to stopping the search session?)}.\footnote{This searcher model can also be represented as a flowchart, as illustrated in Figure~\ref{fig:baskaya_model_flow}.} Indeed, the typical browsing model components (e.g. inspecting a document for relevance) as per~\cite{carterette2011effectiveness_evaluation} are also integrated with query generation and utility accumulation, permitting a searcher to issue multiple queries and gain utility.

\begin{figure}[t!]
    \centering
    \resizebox{1\hsize}{!}{
    \includegraphics{figures/ch3-baskaya_flow.pdf}}
    \caption[Flowchart of the search process by~\cite{baskaya2013behavioural_factors}]{The searcher model, as outlined by~\cite{baskaya2013behavioural_factors}. Adapted from the version of the model illustrated later in Figure~\ref{fig:baskaya_model}, this flowchart illustrates the main processes, decisions and interaction flow that an individual subscribing to this model will follow.}
    \label{fig:baskaya_model_flow}
\end{figure}

Judging result summaries separately from associated documents is to be regarded as a major development in searcher models. The snippet text forming part of each result summary can be generated in a variety of different ways, with approaches today typically \emph{query biased} in nature~\citep{tombros1998query_biased}. As such, what may look like a result summary that is attractive enough to explore further could potentially lead to a document that is in actuality not pertinent to the given information need. This is a clear development from the~\gls{acr:trec}-style searcher model which doesn't consider the notion of a result summary.

Effectively modelling stopping behaviour is also important to provide a more realistic model of the search process. The addition of decision points allowing a searcher to judge the attractiveness of a result summary, or the relevancy of a document provides natural locations for \emph{stopping decision points} (refer to Section~\ref{sec:stopping_background:why:points}).

\begin{quote}
\emph{Given this snippet looks unattractive and therefore not useful for satisfying my information need, should I stop examining the results presented to me on this~\gls{acr:serp}?}
\end{quote}

\begin{figure}[t!]
    \centering
    \resizebox{1\hsize}{!}{
    \includegraphics{figures/ch3-thomas.pdf}}
    \caption[Model of the search process by~\cite{thomas2014modelling_behaviour}]{A model of the search process, considering the high-level processes undertaken by a searcher, as outlined by~\cite{thomas2014modelling_behaviour}. Figure adapted, with the support of the authors. Note here the inclusion of the \genericblack{Select System} process, where a searcher will consider what retrieval system to use before beginning the~\gls{acr:iir} process.}
    \label{fig:thomas_model}
\end{figure}

As an example, a searcher may issue a query that returns very few relevant documents. Once a few snippets and/or documents have been examined, he or she will then conclude that the issued query was unsuccessful and that they would be wasting their time examining more content on the presented~\gls{acr:serp}. In this case, issuing a reformulated query would be a better course of action. This intuition has been confirmed by empirical analysis, where~\cite{azzopardi2011economics} demonstrated that simulated searchers examined significantly fewer documents when the underlying search engine failed to retrieve any relevant material in the top ten results, in contrast to when they did. Thus, searchers are inherently adaptive, with their behaviours conditioned upon the quality of the ranked lists with which they interact. This justifies the inclusion of these additional stopping decision points.

A further explicitly defined searcher model was proposed by~\cite{thomas2014modelling_behaviour}. This model, illustrated as a flowchart in Figure~\ref{fig:thomas_model}, encapsulates the same principles as the previously described searcher models: given an information need, a searcher will issue one or more queries and examine a varying number of result summaries and documents for attractiveness and relevance in turn. This searcher model also includes a form of utility accumulation model that considers the utility obtained from a result summary $s_i$ and the relevancy judgement of a document, $r_i$. An addition to this searcher model is the inclusion of an additional step at the beginning of the search session, where a searcher must decide \emph{what retrieval system to use} -- or \emph{tool selection.}

%This may be dependent upon the task at hand: a patent retrieval system would be a better bet for finding associated patents as opposed to a general web retrieval system, for example.

The searcher model proposed by~\cite{thomas2014modelling_behaviour} also considers the examination of result summaries from an alternative perspective. Instead of assuming that a searcher (perhaps na\"{i}vely) examines each individual result summary in detail before making a decision about its attractiveness, searchers subscribing to this searcher model undertake an \emph{initial inspection} of result summaries, where they can examine in either direction on the list of ranked results (by increasing or decreasing some positional counter, $i$). Essentially, this can be regarded as a searcher \emph{skimming} result summaries, determining if they are worthy of further examination, and adds an additional layer to the search process.

Despite the slight variations in how different researchers interpret the~\gls{acr:iir} process, what can be clearly seen from an examination of these searchers models is the similar workflow between each of them. From query issuance to document examination and judgement, this has emerged as the generally accepted process of~\gls{acr:iir}. While studies examining explicit searcher models are not common, searcher models have been encoded within many of the different evaluation measures used within~\gls{acr:ir} and~\gls{acr:iir}. Moving onto the final section of this background chapter, this is now the topic of our attention.

\section{Evaluation Measures}\label{sec:ir_background:evaluation}
With the basics of~\gls{acr:ir} and~\gls{acr:iir} discussed, we now turn our attention towards the \emph{evaluation} of both a retrieval system and the searchers that use it. Careful and thorough evaluation of retrieval systems is required to demonstrate the superior performance of new retrieval models or a new way of presenting of results, for example~\citep{manning2008ir}. Recalling that the \emph{modus operandi} of a retrieval system is to satisfy the information needs of the searchers who utilise it,~\cite{lancaster1968information} provided three criteria by which an~\gls{acr:ir} system can be evaluated:

\begin{itemize}
    \item{the suitability of a retrieval system in terms of the specific tasks for which it is used;}
    \item{the retrieval system's task performance efficiency; and}
    \item{the extent to which the retrieval system satisfies the information needs of its searchers.}
\end{itemize}

These three criteria may themselves may be, in turn, split into two separate categories: approaches that consider:

\begin{itemize}
    \item{how well the \emph{system} performs, considering the first two criterion above; and}
    \item{how the system performs in the eyes of the \emph{user} who is using the retrieval system at the time~\citep{voorhees2005trec_book}.}
\end{itemize}

Measures for both system and user-focused evaluation are considered in this section, in Subsections~\ref{sec:ir_background:evaluation:system} and~\ref{sec:ir_background:evaluation:user} respectively.

\blueboxheader{Other Evaluation Measures}
We acknowledge that the measures discussed in this section are only a subset of the wider, full range of measures that have been developed and trialled in~\gls{acr:ir} and~\gls{acr:iir} research. For a more comprehensive summary of different evaluation measures, refer to~\cite{sanderson2010test}.

\subsection{System-Based Evaluation}\label{sec:ir_background:evaluation:system}
Considering system-orientated measures of evaluation, one can consider a system's \emph{efficiency} or \emph{effectiveness}. Efficiency concerns some form of operational metrics, such as the speed of the retrieval system. This example is important (especially in commercial retrieval systems) as even a fractional increase of the time taken to return results to a searcher can reduce the number of returning searchers, thus impacting upon the amount of revenue generated through advertising~\citep{brutlag2009speed}.

However, when one thinks of the evaluation of a retrieval system, its \emph{effectiveness} is typically investigated. Of course, the definition of what defines a retrieval system to be effective hugely depends upon the type of search task being undertaken. A patent searcher would, for example, expect a retrieval system to return \emph{all} relevant patents to avoid missing a related patent (and thus incurring penalties). However, a casual web searcher curious about a topic they know little about (i.e. searching in an \emph{ad-hoc} fashion) may be satisfied with either a singular or multiple relevant results.

As such, this section provides a brief overview of basic effectiveness measures widely used within~\gls{acr:ir} today from the perspective of system-sided evaluation -- or the evaluation of the ranked list of results.

\subsubsection{Precision}\label{sec:ir_background:evaluation:system:precision}
The \emph{precision} ($P$) of a ranked list of results is the fraction of documents that have been retrieved that are considered to be relevant or useful for the searcher's information need. A retrieval system that yields high precision is regarded as one that performs well.

Figure~\ref{fig:pr} provides a visual illustration of what precision entails (as well as its counterpart, \emph{\gls{glos:recall},} which is discussed below). Given the set of all documents within an index, a retrieval system will retrieve a number of these documents that satisfy the criteria set out in the employed retrieval model. Of the documents retrieved, some will be considered relevant to the searcher's information need; others will be considered non-relevant. As such, prior knowledge as to what documents are relevant to a given topic are therefore required --~\gls{acr:trec} QRELs are an example of this, as discussed in Section~\ref{sec:ir_background:paradigms:trec}.

Precision is defined as:

\begin{equation}
P = \frac{|~\text{\emph{relevant documents}}~\cap~\text{\emph{retrieved documents}}~|}{|~\text{\emph{retrieved documents}}~|}.
\end{equation}

\gls{acr:ir} research reports precision up to a particular rank, i.e. \emph{\gls{acr:patk}}. For example, $P@10$ will provide a fractional value for the number of relevant documents that appeared within the top $10$ results for a given query. Herein lies one of the most elementary and basic \emph{stopping models} that we find implicitly encoded within various~\gls{acr:ir} measures. Stopping at a depth of $10$, or $k$ (refer to\footnote{The value of $k=10$ is often chosen in~\gls{acr:ir} research as it has been shown that this is typically the depth to which searchers would look through web search results, perhaps due to the effects of pagination~\citep{jansen2006www}.}) denotes that documents past this rank are not examined.

\subsubsection{Recall}\label{sec:ir_background:evaluation:system:recall}
While precision considers the fraction of documents retrieved that are relevant, \emph{\gls{glos:recall}} ($R$) considers the fraction of documents that were retrieved and relevant to a query against \emph{all known relevant documents for a query.} Recall can formally be defined as:

\begin{equation}
R = \frac{|~\text{\emph{relevant documents retrieved}}~|}{|~\text{\emph{relevant documents}}~|}.
\end{equation}

Considering the patent searching example defined above, a high recall would be more desirable in a patent searching task. This means that more patents matching the searcher's query will be returned, thus reducing the possibility of missing important prior, relevant filings.

\begin{figure}[t!]
    \centering
    \resizebox{1\hsize}{!}{
    \includegraphics{figures/ch2-pr.pdf}}
    \caption[Example illustration of precision and recall]{An illustration of precision and recall. On the left is an illustrated example of an index, containing many documents. The large circle represents the set of documents retrieved for a query. Documents that are relevant to the query are represented as~\includegraphics[height=\fontcharht\font`\d]{figures/ch2-pr-r.pdf}, with non-relevant documents represented as~\includegraphics[height=\fontcharht\font`\d]{figures/ch2-pr-nr.pdf}. Note that not all relevant documents are retrieved; doing so would mean that the retrieval engine used would have produced perfect results. From the illustration on the left, definitions of \emph{precision} and \emph{recall} are also provided.}
    \label{fig:pr}
\end{figure}

Given more modern retrieval models, the notion of ranking would lead a searcher to assume (as per the~\gls{acr:prp}~\citep{robertson1977prp}) that relevant documents pertaining to their query will be the first results presented. Non-relevant documents will also of course appear, typically leading to some form of a tradeoff between precision and recall, as discussed below. The tradeoff considers the notion that as you increase recall, the number of non-relevant items will undoubtedly also increase, thus reducing the overall precision of the retrieval system.

% \subsubsection{Average Precision}
% Considering Figure~\ref{fig:ranking} that presents two lists of rankings -- with a~\includegraphics[height=\fontcharht\font`\d]{figures/ch0-tick.pdf}~denoting relevant items, and a \includegraphics[height=\fontcharht\font`\d]{figures/ch0-cross.pdf}~~denoting non-relevant items -- what list would be more desirable? An obvious answer would be the list presented on the right, as the relevant documents are positioned at the top of the rankings. However, if we computed the $P@5$ score for both lists, they would be identical -- $2/5 = 0.4$. Precision offers no discernible difference.
%
% \begin{figure}[t!]
%     \centering
%     \resizebox{1\hsize}{!}{
%     \includegraphics{figures/ch2-ranking.pdf}}
%     \caption[The importance of ranking]{Given an information need to find out information about the \emph{Sir Alwyn Williams Building} (part of the University of Glasgow's estate portfolio), which ranked list of results would you prefer to make use of? Given \emph{positional bias}~\citep{craswell2008click_models}, it is highly likely that searchers would consider the ranked list on the right~\gls{acr:serp} to be preferable.}
%     \label{fig:ranking}
% \end{figure}
%
% One of the most widely used measures that addresses this issue -- and provides a means of quantifying the all-round effectiveness of a retrieval system for a representative set of test topics -- is \emph{Mean Average Precision (MAP).} Rather than considering a single set of results, MAP considers multiple queries -- and, as alluded to above, considers the position at which relevant results appear in the rankings provided.
%
% As the name might imply, MAP provides a mean over the \emph{Average Precision (AP)} for each topic that is examined. AP provides the rank sensitive component of MAP up to some cutoff $k$, as defined below:
%
% \begin{equation*}
% AP = \frac{\sum^{n}_{k=1}(P(k) \cdot rel(k))}{|~\text{\emph{relevant documents}}~|}.
% \end{equation*}
%
% Above, $P(K)$ is the precision cutoff at rank $k$ (i.e. $P@k$), and $rel(k)$ is a function to denote the relevance of a particular document at rank $k$. This is assumed to be binary, with $1$ denoting a relevant document, and $0$ denoting a non-relevant document. AP is then computed for each sample query, $q$, over all of the topics defined in set $T$ to produce a final MAP score for the retrieval system:
%
% \begin{equation*}
% MAP = \frac{\sum^{T}_{q=1} AP(q)}{|~Q~|},
% \end{equation*}
%
% where $Q$ denotes the set of queries that are to be considered in the evaluation.

\subsubsection{Expected Search Length}\label{sec:ir_background:evaluation:system:esl}
The~\gls{acr:esl}~\citep{cooper1968expected_search_length} of a searcher considers the number of non-relevant documents that a searcher will have to search through to obtain the \emph{desired number of relevant documents.} Therefore, systems demonstrating a shorter~\gls{acr:esl} are considered to be more effective than systems with a comparatively longer~\gls{acr:esl}. In other words, the~\gls{acr:esl} indicates how much wasted search effort one would expect using a given retrieval system, as opposed to randomly searching until the needed relevant documents are found.

From~\cite{cooper1968expected_search_length}, the expected search length can be computed thus:

\begin{equation}
ESL(q) = j + \frac{i \cdot s}{r + 1}
\end{equation}

\noindent
where $q$ represents some query and $j$ represents the number of non-relevant documents when considered to query $q$. This is the minimum number of documents that must be found in order to satisfy the searcher's information need -- this is the minimum number before $s$ of the $r$ total relevant documents are found by the searcher.

While we do not explicitly use this measure in the contributory work of this thesis, the~\gls{acr:esl} does nevertheless provide motivation for a number of different stopping heuristics, as outlined in Section~\ref{sec:stopping_background:heuristics}.

\subsubsection{Cumulative Gain Measures}\label{sec:ir_background:evaluation:system:cg}
An important suite of measures that address the utility (or \emph{gain}) that is presented in a ranked list can be derived from~\glsfirst{acr:cg}. Outlined by~\cite{jarvelin2000cg, jarvelin2002cg},~\gls{acr:cg} is measured as the cumulation of gain of all relevant documents up to some rank k (or $CG@k$).~\gls{acr:cg} can be measured on a system-sided basis, purely considering the ranking provided. Alternatively, it can also be measured from a user-sided stance, considering only the documents that a searcher has identified (or saved) as relevant to their information need.

Determining a value for the level of gain that can be acquired from a document is not trivial; experimentation in~\gls{acr:iir} (such as the simulation of interaction study by~\cite{pakkonen2015behavioural_dimensions}) typically uses the~\gls{acr:trec} relevance judgement score for a given topic and document combination as the level of gain that is accrued from a document.

This is demonstrated by the illustration below, highlighting the concept of \emph{graded relevance judgements}. When not related to the information need, a score of $0$ is assigned, with a relevant document assigned a judgement of $2$. Documents partially fulfilling the relevance requirements are assigned an intermediary score of $1$. These values are then accumulated to yield the~\gls{acr:cg} measure for a searcher's interactions.

\vspace{5mm}
\begin{figure}[h!]
    \centering
    \resizebox{1\hsize}{!}{
    \includegraphics{figures/ch2-graded.pdf}}
    \label{fig:graded}
\end{figure}

Relevance judgements can also be indicative of a searcher's stopping behaviour. Assuming a perfect ranking, a searcher would be wise to stop his or her interactions with a ranked list once documents that yield no gain begin to appear as they traverse the ranked list. Continuing examination of the ranked list from that point on would be a waste of their time. Given the illustration in Figure~\ref{fig:cg} however, it is unlikely a perfect ranking will occur in reality, and thus searchers need to determine whether they should continue examining a ranked list after encountering several documents yielding no gain in a row.

Given the definition above,~\gls{acr:cg} can be computed as:

\begin{equation}
    CG_k = \sum_{i=1}^{k} rel_i,
\end{equation}

where $k$ denotes the ranking at which~\gls{acr:cg} should be calculated, and $rel_i$ denotes the relevance assessment score for a document at rank $i$. Given this definition, a computed~\gls{acr:cg} value will be unaffected by changes in rankings. Two lists of rankings with one highly relevant document in rank $1$ in one list, and rank $5$ in the other, will yield identical~\gls{acr:cg} values. As such~\gls{acr:cg} can be essentially considered as $R@k$.

\begin{wrapfigure}[13]{r}{0.45\textwidth}
    \begin{center}
    \vspace*{-10mm}
    \includegraphics[width=1\textwidth]{figures/ch2-cg.pdf}
    \end{center}
    \vspace*{-4mm}
    \caption[Example plot of~\gls{acr:cg} accumulation]{An illustration of the~\glsfirst{acr:cg} as outlined by~\cite{jarvelin2000cg, jarvelin2002cg}. In the example, the first five documents from a list are shown \textemdash~\emph{1,} \emph{3} and \emph{4} contain information, from which the user gains from.}
    \label{fig:cg}
\end{wrapfigure}

A further development of~\gls{acr:cg} is~\gls{acr:dcg}, which, as the name may imply, \emph{discounts} the level of gain accrued by searchers the further down the ranked list of results that they go, thus addressing the problem of~\gls{acr:cg} by increasing the accuracy of the reported values. Specifically, the gain obtained by a new document is discounted according to the rank of that document (i.e. \emph{weighted precision}). A new relevance score is therefore computed for each document by dividing the relevance assessment score by the $log$ of its rank. Further developments have included measures such as \emph{Normalised Discounted Cumulative Gain (nDCG),} which addresses the issue of different result lists having different lengths. To address this, nDCG considers the \emph{ideal DCG (iDCG)} of all relevant documents in a corpus, where the relevance judgements of these documents are ordered by decreasing relevance (i.e. $2,2,2,1,0$). Dividing the DCG score for a ranked list up to rank $k$ by the iDCG score up to rank $k$ thus normalises the reported value across queries.

\subsubsection{Rank-Biased Precision}\label{sec:ir_background:evaluation:system:rbp}
\glsfirst{acr:rbp}~\citep{moffat2008rbp} is a more contemporary~\gls{acr:iir} measure that is derived from a simple searcher model. It considers the notion that searchers are not willing to examine every result presented to them in a ranked list. The encoded searcher model, illustrated in Figure~\ref{fig:rbp}, assumes that a searcher will always examine the first result presented to him or her. The process can be likened to the previously discussed idea of using a Markov model to represent the search process~\citep{tran2017markov_models}, modelling the likelihood of a searcher reaching a given depth. Subsequent documents further down the ranking will then be examined with a decreasing likelihood. Given a ranked list of documents, $d$,~\gls{acr:rbp} is defined as:

\begin{equation}
RBP = (1 - p) \cdot \sum_{k=0}^{d}rel_k \cdot p^{k-1},
\end{equation}

with $rel_k$ once again denoting an assessor's relevance judgement for the document at rank $k$, and $p$ denoting the \emph{patience parameter.} It is this parameter that provides a decaying probability that a searcher will continue to examine a ranked list of results the further down the rankings they go. A searcher subscribing to the~\gls{acr:rbp} searcher model will stop examining results with with probability $1-p$. The patience parameter allows for a very flexible measure; one can model both very persistent searchers (when $p$ tends towards $1.0$) and impatient searchers (where $p\approx0.5$)~\citep{moffat2008rbp}. When $p=0.0$, a searcher will examine only the first presented result for relevance, and then stop.

\gls{acr:rbp} has been shown to fit well with actual searcher data extracted from click logs, as demonstrated, for example, by~\cite{chapelle2009rbp} and~\cite{zhang2010click_rbp}.~\gls{acr:rbp} is used within this thesis as a means for attempting to decide when a simulated searcher is to stop examining a list of ranked results. By incorporating an additional probability in conjunction with the calculated~\gls{acr:rbp} score at some rank $k$, we can determine whether the searcher should stop or continue.

\begin{figure}[t!]
    \centering
    \resizebox{1\hsize}{!}{
    \includegraphics{figures/ch2-rbp.pdf}}
    \caption[Flowchart depicting the searcher model encoded within~\gls{acr:rbp}]{An illustration of the simple searcher model encoded within~\glsfirst{acr:rbp}. A user following this model will \emph{always} examine the first result presented, and will examine subsequent results with decreasing probability (from \emph{p}, the \emph{patience} parameter). Stopping is determined with probability \emph{p-1}. Figure adapted with support from~\cite{moffat2008rbp}.}
    \label{fig:rbp}
\end{figure}

\subsection{User-Based Evaluation}\label{sec:ir_background:evaluation:user}
With basic evaluation measures above considering the system-sided aspects (i.e. the quality of the retrieved ranked list), this section considers a number of~\gls{acr:iir} evaluation measures related to the searcher and their interactions. It may be interesting to note that a large number of measures for~\gls{acr:iir} have been proposed over the years (and indeed, over the experimental spectrum as shown in Figure~\ref{fig:spectrum}); however, only a small number of measures have been regularly used. A more in-depth summary of~\gls{acr:iir} measures can be found in works by~\cite{su1992iir_measures} and~\cite{kelly2009iir}.

Indeed,~\cite{kelly2009iir} provides a taxonomy of the different types of measures used within~\gls{acr:iir} experimentation. The taxonomy consists of four main categories that are enumerated below.

\begin{itemize}
    \item{\blueboxbold{Contextual Factors} These measures are related to the \emph{subject,} or participant, of an~\gls{acr:iir} experiment. Factors include those commonly gathered from forms of demographics surveys, such as the subject's age, sex, and prior search experience. In an information seeking context, one will also be able to gather information about the prior knowledge of the searcher and their knowledge. For example, questions can be asked about their knowledge of the topic about which they have been asked to find information.}
    
    \item{\blueboxbold{Interactions} These measures characterise the interactions that take place between the system and the searcher -- including their behavioural characteristics. Examples of such interactions include: the number of queries that the searcher issues; the number of documents that they examine; the depth to which they examine results (stopping depth); and the mean length (in terms) of the queries that they issue. Time-based measures are also included in this category -- both at a gross level (i.e. the total session time), and at a more specific level (i.e. the mean time spent entering queries). These measures can be usually computed by extracting and parsing interaction \emph{log data}.\footnote{By \emph{log data,} we refer to the file that is created by an experimental retrieval system as a subject conducts a search task. Typically, a series of different actions (i.e. issuing a query, or clicking a document link) are logged, and post-hoc log analysis can then extract this data.}}
    
    \item{\blueboxbold{Performance Factors} As the name suggests, these measures are related to the outcome of the interactions that take place between the searcher and the retrieval system. These measures can be considered analogous to the system-sided measures that we examined previously in Section~\ref{sec:ir_background:evaluation:system}, but with an emphasis on a searcher's interactions (i.e. considering only documents explicitly identified). As such, these measures can again be extracted from interaction log data.}
    
    \item{\blueboxbold{Usability Measures} These measures are typically a series of qualitative and quantitative approaches for capturing a subject's feelings and attitudes towards a search system that they have just used. Common measures in this category include the subject's view of the system's effectiveness and their overall \emph{satisfaction} with how they performed when undertaking the search task in question~\citep{hornbaek2006usability}.}
\end{itemize}

In this thesis, we employ measures from \emph{all four} of the above -- including the extraction of interaction data from user studies, which is subsequently employed to \emph{ground} simulations of interaction. Measures include a variant of~\gls{acr:cg}, for example, that considers the documents that a searcher identifies as relevant during a search session to compute the level of gain that he or she experienced. This section also discusses an additional measure, considering again the interactions that take place between the searcher and retrieval system.

% This section considers five different measures that are used throughout the evaluation of the contributory work reported in this thesis, which are:
%
% \begin{itemize}
%     \item{\blueboxbold{interactive precision} and \blueboxbold{interactive recall}, variants of their system-focused namesakes that consider a searcher's interactions;}
%     \item{\blueboxbold{\glsfirst{acr:cg}}-based measures, including a widely-used and extensively trialled variant, \blueboxbold{\glsfirst{acr:dcg}}; and}
%     \item{\blueboxbold{\glsfirst{acr:rbp}}, considering a decreasing probability of interaction the further down a list of ranked results}
%     %\item{\blueboxbold{INST}, a more complex user-sided evaluation measure.}
% \end{itemize}
%
% We now take each of these measures in turn, discussing the rationale behind each.

\subsubsection{Interactive Precision and Recall}\label{sec:ir_background:evaluation:user:ipr}
During the~\gls{acr:iir} process, searchers examine a number of different result summaries (and their associated documents), making individual judgements as to the relevance of each. Some may not be relevant and are disregarded by the searcher. This differs from the~\gls{acr:trec}-style searcher model that assumes that all documents returned are considered relevant.

Subjects of~\gls{acr:iir} studies are typically instructed to \emph{save} documents that they consider relevant to a provided information need. As such, the judgements created by subjects of~\gls{acr:iir} studies may not match with those made by the assessors of relevance judgements, provided as part of a test collection. Indeed, some~\gls{acr:trec} topics that are widely used contain hundreds of documents marked as relevant by assessors. It is unlikely that a subject would be able to find \emph{all} of these documents or agree with an assessor's judgement.

\begin{figure}[t!]
    \centering
    \resizebox{1\hsize}{!}{
    \includegraphics{figures/ch2-interactive-precision.pdf}}
    \caption[Comparison of precision and interactive precision]{A graphical example of \emph{interactive precision} alongside traditional, system-sided precision. In the example ranking, the retrieval system returned three relevant documents, yet the searcher only correctly identified two of them, hence an interactive precision \emph{(iP)} score of \emph{2/4=0.5.} This is in contrast to the system's precision score of \emph{3/4=0.75.}}
    \label{fig:interactive_precision}
\end{figure}

\glsfirst{acr:ip} and \emph{interactive recall} were defined by~\cite{veerasamy1996iir} and~\cite{veerasamy1997graphical_display}. Instead of purely considering precision and recall as measures exclusively utilising the relevance assessments provided as part of a test collection, one would also consider the number of documents considered relevant by a subject of an~\gls{acr:iir} study that were also \blueboxbold{\gls{acr:trec} relevant}. This means that a document judged relevant by an assessor may not be retrieved, viewed and subsequently judged by the subject of an~\gls{acr:iir} study. We demonstrate this with a visual example in Figure~\ref{fig:interactive_precision}. The \genericblack{Assessors} column denotes the judgement from the relevance assessors, while the \genericblack{Searcher} column denotes the judgement of the searcher. In the example illustration, the retrieval system's $P@4$ score is $3/4=0.75$, while the searcher's $iP@4 = 2/4 = 0.5$. Despite the searcher saving three documents as relevant, the document at rank two was not assessed as such, resulting in the iP score being $0.5$. Therefore, only two of the three saved documents contribute.

% \vspace{-4mm}
% \subsubsection{INST}
% The final measure that we consider is the more recently defined \emph{INST} measure, as described by~\cite{bailey2015inst} and~\cite{moffat2015inst}. INST considers the \emph{conditional probability} that a searcher, having examined a document at rank $k$, will examine the next document at rank $k+1$. This is primarily driven by considering the
%
% \begin{quote}
%     \emph{``number of useful pages that the searcher expects they will need''.}
%     \attrib{\cite{moffat2015inst}}
% \end{quote}
%
% In other words, a searcher would expect, when subscribing to INST, to find $T$ relevant documents in order to satisfy their underlying information need. This measure is again used as a means of determining a searcher's stopping behaviour in subsequent experimentation reported in this thesis.

\subsection{Evaluation Measures and Stopping}
Throughout this section, we have outlined a number of different evaluation measures commonly employed in both~\gls{acr:ir} and~\gls{acr:iir} studies. Common with these measures are implicit models encoded within them that in turn provide a \emph{stopping point} in the ranked list.

These models vary from the simplistic to more complex, with the more complex approaches providing a more realistic rationale of the stopping behaviour of real-world searchers. Regarding stopping behaviours, the most simplistic approach discussed in this chapter is $P@k$ -- or \emph{stop at rank $k$.}. This is agonistic of the relevance of the results presented, and is often described as a fixed-depth assumption, something that we discuss later in Section~\ref{sec:strategies:fixed}. Other measures such as~\gls{acr:rbp}~\citep{moffat2008rbp} and nDCG~\citep{jarvelin2002cg} consist of a more complex stopping model. These measures consider a decreasing likelihood of continuation the deeper a list of results is traversed.

Evaluation measures are however only a small part of the work undertaken in order to understand the different \emph{stopping behaviours} exhibited by searchers during the~\gls{acr:iir} process. Chapter~\ref{chap:stopping_background} provides an in-depth overview of prior work examining this area.

\vspace*{-2mm}
\section{Chapter Summary}
In this chapter, we have introduced some of the key constructs and components of an~\gls{acr:ir} system -- from the document indexing process to the retrieval models that are used to return an often ranked list of results to the searcher. We also briefly touched on the history of the field, discussing some of the key manual and mechanised systems that were commonplace before the advent of computers and the~\gls{acr:www}, giving rise to contemporary~\gls{acr:ir} systems.

The focus of this chapter has however mainly been on the disconnect between traditional~\gls{acr:ir} research, and the reality of what searchers actually do during the~\gls{acr:iir} process. In particular, we identified a number of limitations within the~\gls{acr:trec}-style searcher model that is commonly employed, and discussed a number of more advanced searcher models that reduce the assumptions (and therefore) limitations that the models provide, particularly in terms of a searcher's \emph{interactions.} This led to a discussion of how~\gls{acr:ir} systems and the searchers that use them can be evaluated, along with a discussion of the different categories of study in~\gls{acr:ir} and~\gls{acr:iir} -- from \emph{system-focused} to \emph{user-focused.}

The next chapter continues the focus towards the user, considering previous work that has been undertaken to examine \emph{stopping in~\gls{acr:iir}.}

\newpage
\thispagestyle{empty}
\mbox{}