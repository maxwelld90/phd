%!TEX TS-program = xelatex
%!TEX root = ../../maxwell2018thesis.tex

\chapter[Models and Proposal]{Research Proposal and\\IIR Modelling}\label{chap:proposal}
Having established the prior work that has been undertaken in the field of~\gls{acr:iir} concerning stopping behaviours, we in this chapter outline the approaches we follow in the remainder of the thesis, considering: \emph{(i)} the different stopping strategies that we will employ (Section~\ref{sec:proposal:strategies}); \emph{(ii)} introduce the \emph{Complex Searcher Model (CSM)}, our development of state of the art user models (Section~\ref{sec:proposal:csm}); and \emph{(iii)} provide a detailed explanation of the methodology that we will employ in the remainder of the thesis (Section~\ref{sec:proposal:method}).

\section{Selected Stopping Strategies}\label{sec:proposal:strategies}
As previously illustrated in Section~\ref{sec:stopping_background:heuristics}, a number of different~\glspl{glos:stopping_heuristic} were defined in the information seeking and ecology literature. These were proposed as heuristics which were believed by the authors to quantify and/or explain when searchers stopped searching, or when animals stopped foraging for food~\cite{maxwell2015stopping_strategies}. In this section, we take the rules defined in the literature and operationalise them so that they can be subsequently implemented and tested. Stopping heuristics are operationalised as a series of different~\gls{glos:stopping_strategy}, with each rule assigned an identifier. We split this section into the \todo{seven} broad categories of stopping heuristics that were analysed, with each operationalised stopping strategy listed, with detail provided for the criterion/criteria that must be satisfied before a user employing such a strategy would stop. \todo{This is for snippet level stopping only. Clarify?}

\subsection{Fixed Depth}
The fixed depth stopping strategy is based upon an assumption held across many of the models and measures that are widely used throughout the~\gls{acr:ir} community. This assumption is that a searcher, when examining a list of ranked results for their query, will browse to a \emph{fixed depth} before stopping -- the roots of which can be traced back to the Cranfield Paradigm as discussed in Section~\ref{chap:intro}. Examples of use include the basic stopping model encoded within~\gls{acr:patk}. The assumption is also widely used in the simulation of interaction. For example,~\cite{azzopardi2011economics} conducted a large-scale simulated analysis, where simulated users examined content to depths ranging from $5$ to $1,000$ ($1,000$ is typically assumed in TREC style experimentation, where a single query is issued). Given the wide use of this fixed depth approach in historical and contemporary~\gls{acr:ir} research, we consider this stopping strategy as the baseline approach to which we will be comparing the more advanced, \emph{adaptive} stopping strategies.

\begin{itemize}
    
    \item[]{\blueboxbold{SS1}} Using this stopping strategy, a searcher will stop once they have observed $x_1$ result summaries (i.e. \blueboxbold{SS1} @ $x_1$), regardless of their relevance to the given topic.
    
\end{itemize}

\begin{figure}[t!]
    \centering
    \resizebox{1\hsize}{!}{
    \includegraphics{figures/ch4-ss1.pdf}}
    \caption[Examples of the fixed depth stopping strategy, \blueboxbold{SS1}]{An example of the fixed depth stopping strategy, stylised in this thesis as \blueboxbold{SS1}. Here, a searcher has an information need for the conference \emph{CIKM 2015} in Melbourne, Australia. The left example shows the top five results for poor performing query query, with few useful results (denoted by {\color{dmax_red}crosses}); conversely, the right shows results for a query performing well, with many useful results (denoted by {\color{dmax_green}ticks}). With \blueboxbold{SS1} @4, the searcher will stop at a depth of 4, regardless of the usefulness of the content provided.}
    \label{fig:ss1}
\end{figure}

Given the description of the stopping strategy above, we note that the fixed depth approach is na\"{i}ve in the sense that documents up to rank $x_1$ are useful to the searcher's given information need. On average, such a rule does make sense, but when individual result lists are considered, the approach would not be considered to be a sensible strategy to follow.

Despite the na\"{i}evty of the approach, the other main drawback of such an approach is exposed when a searcher complying with such a strategy issues a poor performing query. This is demonstrated in Figure~\ref{fig:ss1}, with two~\glsplural{acr:serp} presented side by side. Given a searcher's desire to find pages that provide information regarding \emph{CIKM 2015}\footnote{CIKM 2015 was a conference held in Melbourne, Australia, in October 2015. The paper that initially proposed many of these stopping strategies~\cite{maxwell2015stopping_strategies} was indeed presented at this conference.}, two queries are issued: the query on the left yielding poorer results than the query on the right, as denoted by the ticks and crosses, for useful and unhelpful result summaries, respectively. With \blueboxbold{SS1} @4 set, four result summaries are always examined before stopping, regardless of their perceived usefulness. As a result of this, na\"{i}vely examining four documents for the query on the left is by and large a waste of the searcher's time.

\subsection{Tolerance to Non-Usefulness}
The second category of stopping strategies that we propose in this thesis are those that consider a searcher's \emph{tolerance to non-usefulness}. Given a set of result summaries presented on a~\gls{acr:serp}, how many would a searcher be prepared to judge to be of no use before they become frustrated, and subsequently abandon their query?

As detailed in Section~\ref{sec:stopping_background:heuristics}, a number of researchers have proposed stopping heuristics that consider non-usefulness (or \emph{non-relevance}, as defined in the literature). The rule intrinsically makes sense for exhaustive searches~\cite{kraft1979stopping_rules}. As an example, when tasked to find as many documents as possible related to different species of animals that are endangered, becoming disgusted with the presented~\gls{acr:serp} when a lack of new species are shown would be a suitable point at which to break and reformulate a new query, or abandon the search session altogether.

From the heuristics defined by~\citealt{cooper1973retrieval_effectiveness_ii} and~\citealt{kraft1979stopping_rules}, we propose two variants of the disgust rules, \blueboxbold{SS2} and \blueboxbold{SS3}.

\begin{itemize}
    
    \item[]{\blueboxbold{SS2}} Blah
    
    \item[]{\blueboxbold{SS3}} Blah
    
\end{itemize}

Intuitively, these strategies will mean that the simulated searcher adapts their interaction with a ranked list of results depending upon the performance of the underlying query. For example, a ranked list is judged by a simulated searcher as [R,N,N,R,N,N,N], where R and N1 denote relevant and non-relevant items respectively. Under SS2 , with x2 = x3 = 3, the simulated searcher would stop at rank five. Under SS3, the simulated searcher would stop at rank seven.
Since documents are examined when the associated snip- pet is considered relevant (see Figure 1), a simulated searcher may revise their opinion of the amount of relevant informa- tion observed once they see the document. A simulated searcher may for example inspect the first snippet thinking it is relevant, and then examine the document. The docu- ment is subsequently considered non-relevant, meaning the initial R is changed to a N. This updates the count, so that under a stopping strategy with revised relevance, simulated searchers would stop at rank three in the list shown above using SS2 , and similarly for SS3 . This therefore introduces two variants of SS2 and SS3, which were found in a pilot study to perform slightly better. Therefore, we only report the revised relevance variants of SS2 and SS3 in this paper.

\subsection{Considering the Difference}

\subsection{Goal/Satisfaction Based}

\subsection{Time-Based Rules}

\subsection{Combination Rules}

Combination rule by~\citealt{kraft1979stopping_rules} that considers satiation and non-usefulness rules.

Combination rule that is defined in Foraging Theory.

\subsection{Optimal Foraging Rule}

\section{The Complex Searcher Model}\label{sec:proposal:csm}

- we take the models from previous works, and bring them together to produce the complex searcher model.

- we introduce the Complex Searcher Model, derived from previous works.
- based upon probabilities of interaction.

\subsection{CSM Mark I}

\subsection{CSM Mark II}

\subsection{Stopping Decision Points}

\subsection{Considering State and Agency}
- based upon probabilities of interaction
- but this is not the only way in which a user model can be instantiated.
- can develop autonomous agents in which the model is instantiated in such a way so that its decisions are deterministic, not stochastic.
- autonomous stuff is commonplace nowadays, recent example is the rocket launch.
- in search, autonomous agents were commonplace in the mid 2000's, with lots of research going into the exploration of search agency. (brief bit lifted from cikm 2016 paper).

- state is incorporated. indeed, we can argue that even the CSM I/II models consider state too -- but to a lesser degree. for example, the csm is able to consider what documents have been previously clicked on, or saved as useful.

- indeed, this was explored in detail, but is not provided in great depth here.
- refer to Appendix~\ref{appx:agency} for an analysis of incorporating state and agency within the CSM.

- however, this thesis focuses primarily on the notion of stochastic models of interaction, i.e. decision points are determined by the roll of a dice. this in itself introduces a number of additional issues that must be addressed (e.g. the law of averages) which are detailed in Section~\ref{sec:proposal:method:simulations}.

\section{General Methodology Overview}\label{sec:proposal:method}

We want to examine what happens to a searcher's behaviour when different stopping rules are employed under certain contexts.
How can we do this? Here, we provide a broad overview of the methodology that was employed in the later chapters of this thesis. Broadly split across two main sections.

Trial three different contexts to see what happens to searcher behaviour (in particular, stopping behaviour) when subjects to these contexts. In particular, what rules offer the best performance (and approximations)? Helps provide insights into the behavioural changes, and provides concrete evidence 

\subsection{Conducting a User Study}
- run the user study.
- collect the data using the treconomics framework.

\subsection{Extracting User Study Data}
- from the log file, ascertain key data points across the different experimental conditions or interfaces trialled.
- link back to the complex searcher model section to show where each of the probabilities and costs are used/employed.
- then, these costs can be fed into simulations, which can then be run (see Section~\ref{sec:proposal:method:simulations} below).

\subsection{Performing a Simulated Analysis}\label{sec:proposal:method:simulations}
Simulation as defined earlier is a good means for experimentation.
Low cost, always use the same users, no issue of learning bias, etc.

- set up the probabilities
- instantiate the components

- pick a topic set
- pick a corpus
- basic structure of the methodology from previous papers

\subsection{Evaluating Performance and Approximations}
- how do we evaluate how good the system performs?
- how do we work out which one approximates best? on average.



\section{Chapter Summary}
In this chapter we have:

\begin{itemize}
    
    \item{Outlined the different stopping strategies we will employ}
    \item{Discussed the Complex Searcher Model (CSM) that will be used as the foundation to this work; and}
    \item{Provided a high level overview of the methodology that will be employed for the work central to this thesis.}
    
\end{itemize}