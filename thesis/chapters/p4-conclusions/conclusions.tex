%!TEX TS-program = xelatex
%!TEX root = ../../maxwell2018thesis.tex

\chapter[Discussion, Conclusions and Future Work]{Discussion, Conclusions\\and Future Work}\label{chap:conclusions}
With all experimental work now reported, this final chapter provides a conclusion for this thesis. In particular, we provide a high-level summary of the thesis and the reported results, as well a discussion of the results from our simulated analyses. In particular, we emphasise the impact of these findings on~\gls{acr:ir} and~\gls{acr:iir} research. We then outline several potential future research directions, before concluding with some final remarks.

\section{Thesis Summary}\label{sec:conclusions:summary}
In this thesis, we examined how stopping behaviour varies under different search contexts. In particular, we conducted and reported on two user studies under the domain of news search, examining how:~\raisebox{-.2\height}{\includegraphics[height=5mm]{figures/ch2-point1.pdf}} result summary lengths; and~\raisebox{-.2\height}{\includegraphics[height=5mm]{figures/ch2-point2.pdf}} a variation of search tasks, goals and retrieval system affected search behaviours. Result summary interface variations considered a steady increase in the number of snippet fragments considered, all under ad-hoc, time-limited search. This yielded: interfaces \blueboxbold{T0} \emph{(title and no snippets);} \blueboxbold{T1} \emph{(title and one snippet);} \blueboxbold{T2} \emph{(title and two snippets);} and \blueboxbold{T4} \emph{(title and four snippets).} For the latter study reported in Chapter~\ref{chap:diversity}, we considered ad-hoc and aspectual retrieval search tasks (under non-diversified and diversified systems), with \emph{find $x$ relevant} and \emph{find $x$ relevant and new} search goals, without time limits. These were considered under two different systems, one returning results under our baseline BM25 implementation, with the other returning results under BM25 and XQuAD~\citep{santos2010query_reformulations_diversification}. This yielded four conditions, namely: \dualbluebox{D}{AS} \emph{(diversified system, aspectual task);} \dualbluebox{ND}{AS} \emph{(non-diversified system, aspectual task);} \dualbluebox{D}{AD} \emph{(diversified system ad-hoc task);} and \dualbluebox{ND}{AD} \emph{(non-diversified system, ad-hoc task).} Table~\ref{tbl:conclusion_cond_interface_summary} presents a summary table of the different interfaces, systems, and tasks used across the eight variations trialled.

\begin{table}[t!]
    \caption[Summary of experimental interfaces and conditions]{A summary table of the different experimental interfaces and conditions that were trialled. These are based upon the work reported in Chapters~\ref{chap:snippets} and~\ref{chap:diversity}. In total, eight different experimental interfaces and conditioned were employed, considering different result summary lengths, systems and tasks.}
    \label{tbl:conclusion_cond_interface_summary}
    \renewcommand{\arraystretch}{1.8}
    \begin{center}
    \begin{tabulary}{\textwidth}{L{0.4cm}@{\CS}L{3.2cm}@{\CS}D{3.51cm}@{\CS}D{3.51cm}@{\CS}D{3.51cm}@{\CS}}
        & & \lbluecell \textbf{Summary Length} & \lbluecell \textbf{System} & \lbluecell \textbf{Task} \\
        
        \RS \multirow{4}{*}{\rotatebox{90}{\hspace*{-4mm}\textbf{Chapter~\ref{chap:snippets}}}} & \lbluecell\textbf{T0} & \cell \small{Title only} & \cell \small{\blueboxbold{ND} (Non Div.)} & \cell \small{\darkblueboxbold{AD} (Ad-hoc)}\\
        \RS & \lbluecell\textbf{T1} & \cell \small{Title + 1 snippet} & \cell \small{\blueboxbold{ND} (Non Div.)} & \cell \small{\darkblueboxbold{AD} (Ad-hoc)}\\
        \RS & \lbluecell\textbf{T2} & \cell \small{Title + 2 snippets} & \cell \small{\blueboxbold{ND} (Non Div.)} & \cell \small{\darkblueboxbold{AD} (Ad-hoc)}\\
        \RS & \lbluecell\textbf{T4} & \cell \small{Title + 4 snippets} & \cell \small{\blueboxbold{ND} (Non Div.)} & \cell \small{\darkblueboxbold{AD} (Ad-hoc)}\\
        
        \RS\RS\RS \multirow{4}{*}{\rotatebox{90}{\hspace*{-4mm}\textbf{Chapter~\ref{chap:diversity}}}} & \lbluecell\textbf{D-AS} & \cell \small{Title + 2 snippets} & \cell \small{\blueboxbold{D} (Div.)} & \cell \small{\darkblueboxbold{AS} (Aspectual)}\\
        \RS & \lbluecell\textbf{ND-AS} & \cell \small{Title + 2 snippets} & \cell \small{\blueboxbold{ND} (Non Div.)} & \cell \small{\darkblueboxbold{AS} (Aspectual)}\\
        \RS & \lbluecell\textbf{D-AD} & \cell \small{Title + 2 snippets} & \cell \small{\blueboxbold{D} (Div.)} & \cell \small{\darkblueboxbold{AD} (Ad-hoc)}\\
        \RS & \lbluecell\textbf{ND-AD} & \cell \small{Title + 2 snippets} & \cell \small{\blueboxbold{ND} (Non Div.)} & \cell \small{\darkblueboxbold{AD} (Ad-hoc)}\\
        
    \end{tabulary}
    \end{center}
\end{table}

Results from the user studies showed that as result summary lengths increased from \blueboxbold{T0} $\rightarrow$ \blueboxbold{T4}, searchers became more confident in their decisions pertaining to the relevance of documents encountered. However, this was not empirically reflected; indeed, their accuracy in identifying relevant content did not improve with longer result summaries. In terms of stopping behaviours, a downward trend was observed -- as the length of result summaries increased, subjects examined to shallower depths per query. Considering tasks, goals and systems, we found that when using system \blueboxbold{D}, subjects issued more queries and stopped at comparatively shallower depths per query. This is in comparison to system \blueboxbold{ND}, where subjects reported feeling less confident with their decisions. Despite the significant differences we observed in how the two systems performed, we found few significant differences in terms of reported searcher behaviours.

Interaction data from these user studies were then used to ground an extensive set of simulations of interaction. These simulations were designed to test a total of twelve individual stopping strategies, derived from a variety of stopping heuristics\footnote{Stopping heuristics for example considered a searcher's tolerance to non-relevance, or their \emph{frustration} with observing non-relevant content~\citep{kraft1979stopping_rules}.} and~\gls{acr:ir} measures as defined in the literature. Their cataloguing and subsequent operationalisation into stopping strategies provided an answer to \darkblueboxbold{HL-RQ2}. Testing overall performance and how closely the simulations matched up to real-world searcher behaviours across the eight experimental interfaces and conditions, we could then provide answers to both \darkblueboxbold{HL-RQ3a} and \darkblueboxbold{HL-RQ3b}. The simulations were based upon the~\glsfirst{acr:csm}, an updated, high-level conceptual model of the search process. By incorporating a new~\gls{acr:serp} level stopping decision point into the~\gls{acr:csm}, complete with subsequent empirical evaluation (as presented in Chapter~\ref{chap:serp}), we could then provide an answer to \darkblueboxbold{HL-RQ1}.

Results show that when enabled, the new~\gls{acr:serp} stopping decision point leads to significant improvements over the baseline implementation, with consistent improvements in overall performance (measured in~\gls{acr:cg}) reported across a range of experimental conditions, interfaces and stopping strategies. In consideration of approximating real-world searcher stopping behaviours, improvements are also present -- these however were not significantly different. Overall however, these results provide compelling evidence for \darkblueboxbold{HL-RQ1}, and also demonstrate a promising direction for future research in developing our understanding of the search process.



- talk about the serp decision point, show that sig diffs were found.
- in terms of stopping strategies, similar trends were observed across the different conditions and interfaces.
- from the abstract, take the main points.

- a sentence perhaps explaining why it's important work.
- then lead onto the 


in a very short space, what did we examine?

- in this thesis, we examined...
    - how stopping behaviour varies under different search contexts.
    - under the domain of news search, we undertook two different user studies varying things.
    - conditions and interfaces are reported in the table.
    - we then used these to ground a series of simulated interactions.
    - using the complex searcher model.
    - examined a number of different stopping strategies, based upon heuristics from the literature
        - in an attempt to quantify when "enough" information is enough
    - and to see how the interfaces affect searcher stopping behaviours.
    - we also considered a new stopping decision point.
    
    - brief sentence or two for user studies.
    - for the snippets study, people went to greater depths.
    - for the diversity study, more confusing picture.
    
    - for our simulated analysis...
    
    - result showed that... there was little difference between the interfaces and conditions.
    - although for diversified system d, performance was markedly better than when using interface nd.
    
    - fixed depth performed well. counter-intuitive.
    - frustration did really well.
    - combination strategy did well.
    - very similar, very few significant differences.
    - the more complex the strategy, the worse the performance got. we consider this shortly.
    
    - the new stopping decision point was a marked improvement.
    
    - overall, no clear winner in terms of strategies.

\section{Discussion}\label{sec:conclusions:discussion}
so there is the summary.
however, there's a lot of questions about why we got the results that we did.
why is that so?
in this section, we discuss a number of our findings, exploring why the results are what they are.

\subsection{Improving Realism}

- discuss the new decision point.

- chapter 8 simulations...
- however, in terms of performance, the simulated searchers don't do much better on D-AS than D-AD. why?
    - probably because our decision maker didn't help much. doesn't differentiate between relevant and new and relevant.
    - future work. how we instantiate the simulations.
    - maybe this also accounts for why the stopping strategies (or some of them) appeared to underestimate the mean stopping depth. perhaps a better decision maker would be better for this.
    
- does this account for the poor levels of mean cg that are attained in Figure 8.9 compared to the CG attained in Chapter 7?
    - the real-world searchers performed generally better than all of the simulated stopping strategies.
        - what does this suggest? is there another strategy out there that better fits this kind of task?
        - only task that a stopping strategy did offer better approximations for in terms of CG is ND-AS. Which is T2. SS3 does better there. but why?

\subsection{Stopping Strategies and Behaviours}

- simple strategies always perform best.
- the more complex, the worse the performance and approximations get.
    - this could be because of the way we instantiate.
    - IFT/difference.
        - you would expect perhaps with difference, the only strategies that look at the terms, that improvements would be observable as snippet lengths increase. however, this is not the case.

- why does the IFT-based stopping heuristic do so badly? it's the rate of gain computation. our assumptions don't really work over a per-topic basis. think about it this way. consider a topic with a small number of rel documents, and a topic with a large number of rel docs. doesn't matter if it's a good query, it'll be harder to find relevant material for the more difficult topic! so the rate of gain we used doesn't work. so this will be interesting to explore in future work.

- why are the different rules so bad? However, \blueboxbold{QS13} hides the bimodal distribution of performance that would have been experienced during searching. As we examine later, we posit that this may have been detrimental to difference-based stopping strategies \blueboxbold{SS6-DT} and \blueboxbold{SS6-DKL}. This is due to the face that for single term queries, the diversity of the results was likely to be much greater than for three term queries. Consequently, for single term queries that provide little gain, the simulated searcher was likely to have examined to much greater depths.

for frustration rules, as i am more tolerant to nonrel information, i go deeper, has n effect of increased CG.
satisfaction, if i look for one, then stop, tends to be better than going deeper. but kind of surprising.
ift is pretty smooth, following the rate of gain is not the best. maybe it is how we calculate the rate of gain.

- why does RBP underestimate? as you increase the patience parameter, you go deeper down the results. however, as you go deeper, you obtain better performance, but at a diminishing rate. like the difference based strategies, rbp is statistically signficant from ss11. except for T2, where no differences exist. rolling a dice doesnt perform as well. says it has a good user model, but we are showing maybe it doesn't.

- combination strategies do well!
    - people consider how annoyed/satisfied they are. not a single criterion for judging when to stop.
    -

    \begin{table}[t!]
        \caption[GAIN]{GAIN}
        \label{tbl:conclusion_gain}
        \renewcommand{\arraystretch}{1.8}
        \begin{center}
        \begin{tabulary}{\textwidth}{L{0.4cm}@{\CS}L{2.8cm}@{\CS}D{2.6cm}@{\CS}D{2.6cm}@{\CS}D{2.6cm}@{\CS}D{2.6cm}@{\CS}}

            & & \lbluecell \textbf{Time/RS} & \lbluecell \textbf{Time/Doc.} & \lbluecell \textbf{CG} & \lbluecell \textbf{CG/Second} \\

            \RS \multirow{4}{*}{\rotatebox{90}{\hspace*{-4mm}\textbf{Chapter~\ref{chap:snippets}}}} & \lbluecell\textbf{T0} & \cell xx & \cell xx & \cell xx & \cell xx \\
            \RS & \lbluecell\textbf{T1} & \cell xx & \cell xx & \cell xx & \cell xx \\
            \RS & \lbluecell\textbf{T2} & \cell xx & \cell xx & \cell xx & \cell xx \\
            \RS & \lbluecell\textbf{T4} & \cell xx & \cell xx & \cell xx & \cell xx \\
        
            \RS\RS\RS \multirow{4}{*}{\rotatebox{90}{\hspace*{-4mm}\textbf{Chapter~\ref{chap:diversity}}}} & \lbluecell\textbf{D-AS} & \cell xx & \cell xx & \cell xx & \cell xx \\
            \RS & \lbluecell\textbf{ND-AS} & \cell xx & \cell xx & \cell xx & \cell xx \\
            \RS & \lbluecell\textbf{D-AD} & \cell xx & \cell xx & \cell xx & \cell xx \\
            \RS & \lbluecell\textbf{ND-AD} & \cell xx & \cell xx & \cell xx & \cell xx \\
        
        \end{tabulary}
        \end{center}
    \end{table}

\subsection{Simulations of Interaction}

- then move onto a discussion of our simulation findings.
- talk about the different issues that we raised.

- moving to combination approaches, like bejewled~\cite{zhang2017bejewled}, etc.

- talk about the simulations
    - running the simulations. expensive. complicated.
    - as we introduced the SERP level stopping decision point, the complexity increased massively -- and so did the time required to run them.
    - talk about the pre-rolled judgements, too. methodological contribution. ensures repeatable, reproducible research.

- did we do enough runs?
    - we could justify why 50 DM runs was chosen.
    - but was this enough? may not be.
    - maybe we dont have enough trials.
    - and when we dont have enough trials to average over, the power of our experiments could be insufficient.
        - so we could be missing trends. we could be missing things that we simply cannot see.
        - limitation placed upon me by the hardware available to run the experiments. future work.
        - WE MENTION POWER IN SECTION~\ref{sec:methodology:user:flow} -- WITHIN-SUBJECTS INCREASES POWER. BUT WAS IT INCREASED ENOUGH?

would have been better to do it over 50 topics, rather than five.
previous papers show smoother curves.
we tried to compensate for this by doing 50 runs.
but probably not a good drop in solution.
why did we do this though? not enough data. needed to do things in a consistent way.


\section{Conclusion}\label{sec:conclusions:conclusion}

- what is the conclusion?
    - there is no overall winning combination. simple strategies seem to do better in terms of offering outright performance, and approximations. in terms of approximations, tuning a given stopping strategy will yield good approximations, regardless of the strategy you follow. evident with SS1-FIX for example, with comparatively deep depths (i.e. x1=24).

- we also see improvements in modelling.
- at the expense of increased complexity (in terms of running them), the simulations have been shown to be more realistic in terms of approximating click depths.
    - it does appear that taking scent into account is important. leads to slightly lower error rates, although we could not demonstrate this difference to be significant.

- limitations of the work could influence work, hide trends.
- nevertheless, it is an important contribution that demonstrates how task and interface affect stopping behaviour.

\section{Future Research Directions}\label{sec:conclusions:future}
From the discussion and overall conclusions of our results, a number of potential avenues for future work can be considered. In this penultimate section, we consider the possibilities for future work. We present them across four main categories, being: the consideration of stopping strategies and heuristics; the realism of simulations of interaction; how the simulations of interaction are run; and considering future work from the perspective of real-world searchers and search contexts.

\subsection{Stopping Heuristics and Strategies}

\blueboxbold{Stopping Strategy Operationalisation}

\blueboxbold{Additional Stopping Heuristics}
single criterion rule for example.
mental list.
requires more complex implementations. better representations of their cognitive state.

\blueboxbold{Stopping Heuristic Selection}


\subsection{Improving Realism}


\blueboxheader{Improvements to~\gls{acr:csm} Realism}
Addition of extra stopping decision points
MODELS
what about the search context, forgetful brain, for instance?
talk about agents here.
tool choice.
pagination.

\blueboxbold{Improving Measures of Search}
MEASURES

\subsection{Simulations of Interaction}

\blueboxbold{Scaling Simulations of Interaction}
pre-rolled judgements were a good call.
is the power enough? what if we ran more experiments?

\subsection{Contexts and Searchers}

\blueboxbold{Additional Search Contexts}

\blueboxbold{Individual Searcher Stopping}













Additional Stopping Decision point -- improving realism further
Taking the CSM further. Tool switching. Other things to add to the searcher model to hitherto improve realism.

Additional search contexts
Do the same findings apply to different search contexts? We considered only news search. And ad-hoc and aspectual retrieval. To see how things vary. But it is likely that they will vary elsewhere. Context, surroundings, device...

additional stopping heuristics

encoding additional stopping models and measures
    - turning SS5/SS11 into a stopping model. e.g. bejewled, following on that train of thought.

existing stopping strategy operationalisation
We could have operationalised the stopping heuristics in many different ways. We chose simplistic means to operationalise them. These means may or may not have influenced the performance/approximations that each strategy offered. Future work is required to determine if this is indeed the case or not.

There are many, many different avenues for exploration here. Prime example: OFT strategy. Consistently performed poorly. List the different strategies that we could operationalise in a different way, explaining what may happen (educated guess).
For example, OFT, rate of gain set over all topics trialled. But we know that some topics are more difficult to find relevant material for than others (reference the table with the number of rel documents, show that one has less than the other). This would influence stopping, because simply less likely a relevant item would appear in the ranks.

heuristic selection
We didn't really consider under what circumstances one heuristic would be better than another. So, how would one be able to select the best threshold for a given circumstance? Kind of eluded to this with SS11-COMB, that selected a stopping strategy based upon the perceived yield of a SERP. However, are there different factors which could influence the selection of a particular heuristic, and what are they? What heuristics would work best?

individual searcher stopping
How can we use the knowledge reported in this thesis to develop models more in line with an individual's stopping behaviours?
COMBO - recent work has shown that approaches like bejeweled are more in line with what people actually do. evidence from this thesis somewhat supports this argument -- combination approaches do well, and offer good performance. perhaps we need to look away from a single heuristic and consider two (or even more), see heuristic selection above.

Additionally, future work can work on these models at an individual searcher level. This thesis has exclusively reported behaviours on average, across \~ 50 subjects over different interfaces and conditions. This provides us with a rough approximation as to what will work best. However, people behave differently -- there is no general rule of thumb. For example, could we classify people based upon their searcher behaviours? Like Smucker's fast and liberal, slow and strict classification. Perhaps getting a better understanding of their searcher behaviours could factor into the interface used, and then this could improve their searching ability.

realism
We made assumptions for the diversity study. The decision maker was based on ad-hoc retrieval.
The task for two was aspectual. This was not considered, yet we still got good gain and approximations. Would this improve if we undertook work to develop new decision making components, for the different retrieval tasks?

pagination of results. how did pagination affect everything?

We should acknowledge that by not considering each stopping strategy in chapter 9, we might be missing something out. There could be some things at play that we do not consider -- because everything is linked together. 

how does the scent judgement affect how the searcher performs? is p@vize optimal? do different people have different thresholds here?

why did average drop off sometimes in Ch9, falling below Always?

for SS2 and SS5 in Average, why was the mean depth per query higher?

for comparisons in ch9... why did ss1 and ss5 at perfect give the best approximations? that is interesting and slightly odd.

if a searcher performs badly/worse than always, they could be stopping queries early on that are bad, and entering those that are good, saving documents. likelihood that relevant documents already saved will reappear in subsequent queries later on, they have more time to issue more if they skip queries! and the judgement is such that if it's relevant, you go in. no consideration for things observed previously. did not consider this; so you enter a SERP, but there's nothing new to save even though it looked good! mismatch here -- something to address in future work.

scaling experiments up further.
    power idea in discussion.


\section{Final Remarks}\label{sec:conclusions:remarks}
The search process is complex, with searchers able to undertake a wide range of potential actions and decisions. With these actions and decisions, differing behaviours can be observed. We have shown in this thesis that \emph{stopping behaviours} can also be considered as difficult to examine and measure. Nevertheless, this work motivates the fact that we need to consider stopping behaviours in the development of future evaluation measures and search interfaces. Despite the inherently difficult task that understanding and modelling the stopping behaviours of searchers represents, we believe that the potential benefits of further exploration in this area will undoubtedly aid the searchers and researchers of future retrieval systems. Now that this is all done, \emph{I} am off to the pub -- and having reached the end of this thesis, you should go, too!