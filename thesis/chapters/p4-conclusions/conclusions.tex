%!TEX TS-program = xelatex
%!TEX root = ../../maxwell2018thesis.tex

\chapter[Discussion, Conclusions and Future Work]{Discussion, Conclusions\\and Future Work}\label{chap:conclusions}
With all experimental work now reported, this final chapter provides a conclusion for this thesis. In particular, we provide a high-level summary of the thesis and the reported results, as well a discussion of the results from our simulated analyses. In particular, we emphasise the impact of these findings on~\gls{acr:ir} and~\gls{acr:iir} research. We then outline several potential future research directions, before concluding with some final remarks.

\section{Thesis Summary}\label{sec:conclusions:summary}
In this thesis, we examined how stopping behaviour varies under different search contexts. In particular, we conducted and reported on two user studies under the domain of news search, examining how:~\raisebox{-.2\height}{\includegraphics[height=5mm]{figures/ch2-point1.pdf}} result summary lengths; and~\raisebox{-.2\height}{\includegraphics[height=5mm]{figures/ch2-point2.pdf}} a variation of search tasks, goals and retrieval system affected search behaviours. Result summary interface variations considered a steady increase in the number of snippet fragments considered, all under ad-hoc, time-limited search. This yielded: interfaces \blueboxbold{T0} \emph{(title and no snippets);} \blueboxbold{T1} \emph{(title and one snippet);} \blueboxbold{T2} \emph{(title and two snippets);} and \blueboxbold{T4} \emph{(title and four snippets).} For the latter study reported in Chapter~\ref{chap:diversity}, we considered ad-hoc and aspectual retrieval search tasks (under non-diversified and diversified systems), with \emph{find $x$ relevant} and \emph{find $x$ relevant and new} search goals, without time limits. These were considered under two different systems, one returning results under our baseline BM25 implementation, with the other returning results under BM25 and XQuAD~\citep{santos2010query_reformulations_diversification}. This yielded four conditions, namely: \dualbluebox{D}{AS} \emph{(diversified system, aspectual task);} \dualbluebox{ND}{AS} \emph{(non-diversified system, aspectual task);} \dualbluebox{D}{AD} \emph{(diversified system ad-hoc task);} and \dualbluebox{ND}{AD} \emph{(non-diversified system, ad-hoc task).} Table~\ref{tbl:conclusion_cond_interface_summary} presents a summary table of the different interfaces, systems, and tasks used across the eight variations trialled.

\begin{table}[t!]
    \caption[Summary of experimental interfaces and conditions]{A summary table of the different experimental interfaces and conditions that were trialled. These are based upon the work reported in Chapters~\ref{chap:snippets} and~\ref{chap:diversity}. In total, eight different experimental interfaces and conditioned were employed, considering different result summary lengths, systems and tasks.}
    \label{tbl:conclusion_cond_interface_summary}
    \renewcommand{\arraystretch}{1.8}
    \begin{center}
    \begin{tabulary}{\textwidth}{L{0.4cm}@{\CS}L{3.2cm}@{\CS}D{3.51cm}@{\CS}D{3.51cm}@{\CS}D{3.51cm}@{\CS}}
        & & \lbluecell \textbf{Summary Length} & \lbluecell \textbf{System} & \lbluecell \textbf{Task} \\
        
        \RS \multirow{4}{*}{\rotatebox{90}{\hspace*{-4mm}\textbf{Chapter~\ref{chap:snippets}}}} & \lbluecell\textbf{T0} & \cell \small{Title only} & \cell \small{\blueboxbold{ND} (Non Div.)} & \cell \small{\darkblueboxbold{AD} (Ad-hoc)}\\
        \RS & \lbluecell\textbf{T1} & \cell \small{Title + 1 snippet} & \cell \small{\blueboxbold{ND} (Non Div.)} & \cell \small{\darkblueboxbold{AD} (Ad-hoc)}\\
        \RS & \lbluecell\textbf{T2} & \cell \small{Title + 2 snippets} & \cell \small{\blueboxbold{ND} (Non Div.)} & \cell \small{\darkblueboxbold{AD} (Ad-hoc)}\\
        \RS & \lbluecell\textbf{T4} & \cell \small{Title + 4 snippets} & \cell \small{\blueboxbold{ND} (Non Div.)} & \cell \small{\darkblueboxbold{AD} (Ad-hoc)}\\
        
        \RS\RS\RS \multirow{4}{*}{\rotatebox{90}{\hspace*{-4mm}\textbf{Chapter~\ref{chap:diversity}}}} & \lbluecell\textbf{D-AS} & \cell \small{Title + 2 snippets} & \cell \small{\blueboxbold{D} (Div.)} & \cell \small{\darkblueboxbold{AS} (Aspectual)}\\
        \RS & \lbluecell\textbf{ND-AS} & \cell \small{Title + 2 snippets} & \cell \small{\blueboxbold{ND} (Non Div.)} & \cell \small{\darkblueboxbold{AS} (Aspectual)}\\
        \RS & \lbluecell\textbf{D-AD} & \cell \small{Title + 2 snippets} & \cell \small{\blueboxbold{D} (Div.)} & \cell \small{\darkblueboxbold{AD} (Ad-hoc)}\\
        \RS & \lbluecell\textbf{ND-AD} & \cell \small{Title + 2 snippets} & \cell \small{\blueboxbold{ND} (Non Div.)} & \cell \small{\darkblueboxbold{AD} (Ad-hoc)}\\
        
    \end{tabulary}
    \end{center}
\end{table}

Results from the user studies showed that as result summary lengths increased from \blueboxbold{T0} $\rightarrow$ \blueboxbold{T4}, searchers became more confident in their decisions pertaining to the relevance of documents encountered. However, this was not empirically reflected; indeed, their accuracy in identifying relevant content did not improve with longer result summaries. In terms of stopping behaviours, a downward trend was observed -- as the length of result summaries increased, subjects examined to shallower depths per query. Considering tasks, goals and systems, we found that when using system \blueboxbold{D}, subjects issued more queries and stopped at comparatively shallower depths per query. This is in comparison to system \blueboxbold{ND}, where subjects reported feeling less confident with their decisions. Despite the significant differences we observed in how the two systems performed, we found few significant differences in terms of reported searcher behaviours.

Interaction data from these user studies were then used to ground an extensive set of simulations of interaction. These simulations were designed to test a total of twelve individual stopping strategies, derived from a variety of stopping heuristics\footnote{Stopping heuristics for example considered a searcher's tolerance to non-relevance, or their \emph{frustration} with observing non-relevant content~\citep{kraft1979stopping_rules}.} and~\gls{acr:ir} measures as defined in the literature. Their cataloguing and subsequent operationalisation into stopping strategies provided an answer to \darkblueboxbold{HL-RQ2}. Testing overall performance and how closely the simulations matched up to real-world searcher behaviours across the eight experimental interfaces and conditions, we could then provide answers to both \darkblueboxbold{HL-RQ3a} and \darkblueboxbold{HL-RQ3b}. The simulations were based upon the~\glsfirst{acr:csm}, an updated, high-level conceptual model of the search process. By incorporating a new~\gls{acr:serp} level stopping decision point into the~\gls{acr:csm}, complete with subsequent empirical evaluation (as presented in Chapter~\ref{chap:serp}), we could then provide an answer to \darkblueboxbold{HL-RQ1}.

Results show that when enabled, the new~\gls{acr:serp} stopping decision point leads to significant improvements over the baseline implementation, with consistent improvements in overall performance (measured in~\gls{acr:cg}) reported across a range of experimental conditions, interfaces and stopping strategies. In consideration of approximating real-world searcher stopping behaviours, improvements are also present -- these however were not significantly different. Overall however, these results provide compelling evidence for \darkblueboxbold{HL-RQ1}, and also demonstrate a promising direction for future research in developing our understanding of the search process.

With respect to our simulated analyses of individual stopping strategies, we found several stopping strategies offered the highest overall levels of~\gls{acr:cg} and good approximations with actual searcher stopping behaviours. For example, as result summary lengths increased, we found that \blueboxbold{SS11-COMB} consistently offered the best performance, with both \blueboxbold{SS1-FIX} and \blueboxbold{SS4-SAT} offering the best real-world searcher approximations. \blueboxbold{SS5-COMB} also offered the best overall levels of~\gls{acr:cg} across the second user study, with \blueboxbold{SS1-FIX} again offering the best levels of performance across condition \dualbluebox{ND}{AD}. In terms of approximations, \blueboxbold{SS1-FIX} and \blueboxbold{SS10-RELTIME} yielded the lowest MSE values. However, despite these strategies performing well, no one strategy clearly emerged as offering significantly improved levels of performance or approximations. However, several more complex stopping strategies such as \blueboxbold{SS6-DT} and \blueboxbold{SS7-DKL} consistently offered poorer performance and approximations. This is a common theme in our results: simplistic and combination-based stopping strategies generally performed better. This includes the fixed-depth stopping strategy, \blueboxbold{SS1-FIX}, which, counter to our intuition, consistently performed well. These results show that modelling stopping behaviours \emph{is} indeed difficult.

\section{Discussion}\label{sec:conclusions:discussion}
From the analysis of our simulations of interaction, a number of interesting areas of discussion became apparent. In this section, we discuss our findings, providing an emphasis on an examination of the stopping strategies that we trialled. In particular, our discussion is guided by addressing the reasoning behind the answers of the four overarching research questions posed in this thesis.

\begin{itemize}
    \item{\darkblueboxbold{HL-RQ1} How can we improve searcher models to incorporate different stopping decision points?}
    \item{\darkblueboxbold{HL-RQ2} Given the stopping heuristics defined in the literature, how can we encode these heuristics into a series of operationalised, programmable stopping strategies that can be subsequently incorporated into the searcher model and be evaluated?}
    \item{\darkblueboxbold{HL-RQ3a} Given the aforementioned operationalised stopping strategies, how well does each one perform?}
    \item{\darkblueboxbold{HL-RQ3b} How closely do the operationalised stopping strategies compare to the actual stopping behaviours of real-world searchers?}
    
\end{itemize}

These questions relate specifically to the stopping strategies and simulations of interaction that were trialled. For a more general discussion on the results of the two user studies, and how behaviours varied, refer to Sections~\ref{chap:snippets:user:discussion} on page~\pageref{chap:snippets:user:discussion} and~\ref{sec:diversity:users:discussion} on page~\pageref{sec:diversity:users:discussion}.

\subsection{Searcher Models and Realism}

- inclusion of new stopping decision point helps.
- improves the realism of the experiments.

- what about other stopping strategies?
    - reason to believe that they would offer improvements, too.

- probabilities were similar across each of the different SERP decision point interface/conditions.

\subsection{Stopping Strategy Operationalisation}
Our second area of discussion considers the twelve result summary level stopping strategies that we trialled. In particular, we focus on the specifics of our implementations for these strategies, and discuss how our implementations could have affected the overall performance and approximations that they attained. In turn, this sets up discussion for related, future work that we provide in Section~\ref{sec:conclusions:future:stopping}.

In general, findings across both Chapters~\ref{chap:snippets} and~\ref{chap:diversity} demonstrates that stopping strategies offering more simplistic implementations tended to yield better performance, and matched better with real-world stopping behaviours. Strategies \blueboxbold{SS2-NT}, \blueboxbold{SS3-NC}, \blueboxbold{SS4-SAT}, \blueboxbold{SS9-TIME} and \blueboxbold{SS10-RELTIME} all for example performed and approximated well. We consider these simplistic in the sense that the stopping criterion that they each encoded was straightforward to implement, considering aspects such as the number of non-relevant documents encountered, or the elapsed time spent searching from query issuance.

In contrast, findings demonstrated that the more complex stopping strategies tended to perform worse. Strategies implementing the difference heuristic, such as \blueboxbold{SS6-DT} and \blueboxbold{SS7-DKL}, and the optimal stopping heuristic (\blueboxbold{SS8-IFT}) consistently offered poorer performance and approximations -- in some cases, significantly worse than the best for a given interface or condition. These strategies were considered to be more complex because of the criteria they considered -- rather than a simplistic lookup of some basic count of an occurrence, a more series of complex calculations were required in order to determine whether the simulated searcher should stop examining content, or continue.

\begin{figure}[t!]
    \centering
    \resizebox{1\hsize}{!}{
    \includegraphics{figures/ch10-queryperf.pdf}}
    \caption[Query performance by real-world subjects]{Plot and table demonstrating the performance of queries (\genericblack{P@10}) across the four experimental interfaces trialled in the user study we report on in Chapter~\ref{chap:snippets}. Note the relatively high number of queries yielding no relevant results in the top 10, with a similar number reported for queries with \emph{P@10=0.2}. Similar findings were observed for the study reported in Chapter~\ref{chap:diversity}.}
    \label{fig:query_performance_ch7}
\end{figure}

Given these general findings, \emph{why} did the more complex stopping strategies perform and approximate worse on average? We hypothesise several reasons as to why this may have been the case, with future examination required in order to confirm these hypotheses. Taking the difference-based stopping strategies into account first, we hypothesise that the bimodal distribution of performance over querying strategy \blueboxbold{QS13} may have been the reason why the strategies offered poorer performance. Recall that querying strategy \blueboxbold{QS13} issued a single term query, then a three term query. For single term queries, the diversity of the results returned was likely to be much greater than for three term queries. Consequently, for single term queries that provided little potential gain, simulated searchers would likely have examined to much greater depths. This is true also for real-world queries used for the comparison runs (addressing \darkblueboxbold{HL-RQ3b}) -- many poor quality real-world queries were issued, with $P@10=0.0$. This would have likely had a negative influence on how the different stopping strategies performed. To demonstrate this, Figure~\ref{fig:query_performance_ch7} highlights the distribution of performance across the queries issued by subjects from the user study reported in Chapter~\ref{chap:snippets}.

Next, we consider the~\gls{acr:ift}-based strategy \blueboxbold{SS8-IFT}. Evidence has shown that~\gls{acr:ift} has proven good at predicting searcher behaviours, with recent approaches demonstrated by~\cite{ong2017scent_behaviour} and~\cite{azzopardi2018cwl}, for example. Indeed, in Section~\ref{sec:diversity:users:results:ift} on page~\pageref{sec:diversity:users:results:ift}, we observed that our~\gls{acr:ift}-based hypotheses matched up closely with empirical evidence. Why did \blueboxbold{SS8-IFT} then proceed to consistently offer poorer performance and approximations than more simplistic approaches? We hypothesise that this comparative lack of performance can be attributed to how the \emph{rate of gain} is computed, which is the stopping criterion employed in \blueboxbold{SS8-IFT}. The assumptions that we made may not work particularly well over a per-topic basis, with the same rate of gain used across all five topics examined. Table~\ref{tbl:ch6_topic_rels} on page~\pageref{tbl:ch6_topic_rels} demonstrated that the number of~\gls{acr:trec} relevant documents for each topic varies considerably. As such, one would expect that the computed rate of gain should vary, at least on a per-topic basis. This way, expectations of gain can be kept in check -- a rate of gain computation computed over a topic with many~\gls{acr:trec} relevant documents would perform much worse than using the same value over a topic with a comparatively smaller set of relevant documents. This per-topic difference is graphically illustrated in Figure~\ref{fig:per_topic_differences}, using interface \blueboxbold{T2} (left) and condition \dualbluebox{ND}{AD} (right) over \blueboxbold{SS3-NC}. Note the wide variation in performance across individual topics -- and the general trend of higher topic performance if a greater number of~\gls{acr:trec} relevant documents are present.

\begin{figure}[t!]
    \centering
    \resizebox{1\hsize}{!}{
    \includegraphics{figures/ch10-per_topic.pdf}}
    \caption[Per-topic performance variation example]{Plots demonstrating the wide per-topic variance over the \emph{what-if} performance simulations. On the left, performance over interface \blueboxbold{T2} is shown – \dualbluebox{ND}{AD} is shown on the right. Stopping strategy \blueboxbold{SS3-NC} is used for this demonstration. Similar observations could be observed across other interfaces, conditions and stopping strategies. Also \blueboxbold{highlighted} on the left plot is the number of~\gls{acr:trec} relevant documents for each topic. Note the general performance improvement as the number of~\gls{acr:trec} relevant documents increases for a topic.}
    \label{fig:per_topic_differences}
\end{figure}

We also observed that the~\gls{acr:rbp}-based measure, \blueboxbold{SS12-RBP}, also generally failed to perform and approximate well. Even with a high patience parameter $p$, we observed that the stopping strategy continually underestimated the stopping depths of real-world searchers. (Refer to plot \blueboxbold{SS12-RBP} in Figure~\ref{fig:ch7_sim_comparison_plots} on page~\pageref{fig:ch7_sim_comparison_plots} for an example.) As the $p$ parameter increases, the simulated searcher examines contents to steadily increasing depths. The greater a searcher would go, the better their performance would be -- but performance would improve at a diminishing rate. These poorer results demonstrate that rolling a dice (as is the case for \blueboxbold{SS12-RBP}) doesn't necessarily perform well or offer good approximations. This is at odds with prior research demonstrating that~\gls{acr:rbp} offers a good searcher model -- evidence reported in this thesis suggests to the contrary.

Both stopping strategies considering a combination-based approach -- \blueboxbold{SS5-COMB} and \blueboxbold{SS11-COMB} -- performed and approximated well. Results from \blueboxbold{SS5-COMB} in particular seem to suggest that, at least in terms of approximating stopping behaviours, searchers do not consider a single criterion when determining the point at which they should stop examining results. This is an interesting conclusion, with other evidence available to support this. A recent study by~\cite{zhang2017bejewled} used the \emph{Bejeweled Player Model (BPM)} to model a searcher's stopping behaviours, where they would stop when \emph{``he/she either has found sufficient useful information, or no more patience to continue.''} This description is analogous to a combination of stopping strategies \blueboxbold{SS4-SAT} (satiation) and one of either \blueboxbold{SS2-NT} or \blueboxbold{SS3-NC}, with findings from this study showing improvements in correlations between searcher satisfaction and existing measures. In a similar fashion, work by~\cite{azzopardi2018cwl} has also demonstrated that better evaluation measures can be developed by considering not only the gain accumulated, but the cost expended in attaining said gain. The BPM was also demonstrated in this subsequent study to offer superior performance when compared against other established evaluation measures. This mounting evidence suggests that when considering stopping behaviours, evaluation measures should consider stopping from more than one single criterion.

While also regarded as a combination-based strategy, \blueboxbold{SS11-COMB} was operationally different from \blueboxbold{SS5-COMB}. Rather than considering multiple stopping criteria at once, \blueboxbold{SS11-COMB} considered a single stopping strategy, based upon an initial impression of the presented~\gls{acr:serp}. This stopping strategy also performed and approximated well, and provides evidence that rigidly following a single stopping strategy across a range of queries with differing performance may not be the best approach. This approach is intuitive -- picking a satiation-based approach makes sense for a query with a large number of relevant documents. The likelihood of being satisfied would be greater than a query with a comparatively low number of relevant documents. In this case, the give-up time-based approach also intuitively makes sense. However, the good performance offered by this stopping strategy could be partially down to the means by which we determined whether a~\gls{acr:serp} was high yielding early on or not. Employing the use of~\gls{acr:trec} QRELs to determine the $P@1$ judgement meant that the simulated searcher possessed an unfair advantage. If we were for example to introduce a stochastic element to this stopping strategy, we may see lower performance -- but conversely, better approximations. We did not consider this due to limitations of computer hardware, as we discuss in Section~\ref{sec:conclusions:future:running}.

Overall, a majority of stopping strategies performed well and produced approximations that were very close to one another, with few significant differences. One particularly surprising result was that of \blueboxbold{SS1-FIX}. The fixed-depth, non-adaptive approach consistently offered good performance and approximations. This is surprising and counter-intuitive, as it would make sense for more adaptive strategies to reflect closer to real-world searcher stopping behaviours. This may be a result of averaging over a population -- individual stopping behaviours would likely demonstrate that adaptive strategies would outperform \blueboxbold{SS1-FIX}. Nevertheless, many of the stopping strategies trialled, when tuned appropriately (i.e. \dualbluebox{SS1-FIX}{@24}, which one could argue as being wholly unrealistic), offer good approximations and performance.

\subsection{Performance, Interfaces and Conditions}
Considering the eight experimental interfaces and conditions, we found the best approximations were attained over interface \blueboxbold{T2}. Approximations over this particular interface consistently offered the lowest MSE values, demonstrating that the extracted interaction probabilities appeared to do a very good job at estimating real-world examination strategies. Considering the study in Chapter~\ref{chap:diversity}, condition \dualbluebox{D}{AS} consistently offered better approximations than the other three.

More interesting however is the difference in the levels of~\gls{acr:cg} that were attained by simulated searchers if they rigidly followed each of the twelve stopping strategies trialled. This was achieved when the stopping strategies were tuned to the configuration yielding the best approximation to actual searcher stopping behaviours. We demonstrated this difference with the bar charts presented in Figure~\ref{fig:ch7_sim_comparison_rankings} on page~\pageref{fig:ch7_sim_comparison_rankings}, and Figure~\ref{fig:ch8_sim_comparison_rankings} on page~\pageref{fig:ch8_sim_comparison_rankings}. Recall from the plots that several of the simplistic stopping strategies (i.e. \blueboxbold{SS1-FIX}, \blueboxbold{SS2-NT}, \blueboxbold{SS3-NC} and \blueboxbold{SS4-SAT}) typically outperformed the mean real-world searcher~\gls{acr:cg} value attained over each of the four experimental interfaces reported in Chapter~\ref{chap:snippets}. In contrast, when the stopping strategies were run over the experimental conditions in Chapter~\ref{chap:diversity}, results showed the real-world~\gls{acr:cg} values were much better (often as much as one entire unit of gain higher) than the best stopping strategy approximation offered.\footnote{The exception to this trend was \blueboxbold{SS3-NC} over condition \dualbluebox{ND}{AS}, which offered a higher level of~\gls{acr:cg} than the real-world mean.}

\begin{table}[t!]
    \caption[Observed cost per unit of gain]{Summarising statistics across the eight interfaces and conditions trialled. Included in this table is the reported average session time (\genericblack{Avg. S.T.}), mean \genericblack{\gls{acr:cg}} attained, and the cost per unit of~\gls{acr:cg}, or \genericblack{Cost/Unit~\gls{acr:cg}}. All $\pm$ values denote the standard deviations. Values reported are over the real-world subjects of the two user studies conducted, as reported in Chapters~\ref{chap:snippets} and~\ref{chap:diversity}. Values for \genericblack{Chapter~\ref{chap:snippets} (Uncut)} relate to the full ten minutes, rather than the first six minutes used elsewhere. Refer to Section~\ref{sec:snippets:method:subjects} on page~\pageref{sec:snippets:method:subjects} for further information.}
    \label{tbl:conclusion_gain}
    \renewcommand{\arraystretch}{1.8}
    \begin{center}
    \begin{tabulary}{\textwidth}{L{0.4cm}@{\CS}L{3.85cm}@{\CS}D{3.3cm}@{\CS}D{3.3cm}@{\CS}D{3.3cm}@{\CS}}

        & & \lbluecell \textbf{Avg. S.T.} & \lbluecell \textbf{\gls{acr:cg}} & \lbluecell \textbf{Cost/\gls{acr:cg} Unit} \\

        \RS \multirow{4}{*}{\rotatebox{90}{\hspace*{-4mm}\textbf{Chapter~\ref{chap:snippets}}}} & \lbluecell\textbf{T0} & \cell 353.18$\pm$44.22 & \cell 1.87$\pm$2.39 & \cell 188.87 \\
        \RS & \lbluecell\textbf{T1} & \cell 356.18$\pm$41.79 & \cell 1.83$\pm$2.17 & \cell 194.63 \\
        \RS & \lbluecell\textbf{T2} & \cell 366.49$\pm$39.82 & \cell 2.36$\pm$2.28 & \cell 155.29 \\
        \RS & \lbluecell\textbf{T4} & \cell 361.01$\pm$49.33 & \cell 1.87$\pm$1.21 & \cell 193.05 \\
        
        \RS\RS\RS \multirow{4}{*}{\rotatebox{90}{\hspace*{-5mm}\textbf{Chapter~\ref{chap:snippets} (Uncut)}}} & \lbluecell\textbf{T0} & \cell xx & \cell xx & \cell xx \\
        \RS & \lbluecell\textbf{T1} & \cell xx & \cell xx & \cell xx \\
        \RS & \lbluecell\textbf{T2} & \cell xx & \cell xx & \cell xx \\
        \RS & \lbluecell\textbf{T4} & \cell xx & \cell xx & \cell xx \\

        \RS\RS\RS \multirow{4}{*}{\rotatebox{90}{\hspace*{-4mm}\textbf{Chapter~\ref{chap:diversity}}}} & \lbluecell\textbf{D-AS} & \cell 443.65$\pm$45.05 & \cell 4.09$\pm$2.64 & \cell 108.47 \\
        \RS & \lbluecell\textbf{ND-AS} & \cell 430.50$\pm$38.39 & \cell 3.35$\pm$2.92 & \cell 128.51 \\
        \RS & \lbluecell\textbf{D-AD} & \cell 432.18$\pm$49.87 & \cell 4.12$\pm$2.89 & \cell 104.89 \\
        \RS & \lbluecell\textbf{ND-AD} & \cell 447.55$\pm$47.82 & \cell 3.49$\pm$2.76 & \cell 128.24 \\

    \end{tabulary}
    \end{center}
\end{table}

To examine this in more detail, we undertook further analysis of the~\gls{acr:cg} attained by real-world searchers over the eight experimental interfaces and conditions. Table~\ref{tbl:conclusion_gain} reports the mean session time (\genericblack{Avg S.T.}) and \genericblack{\gls{acr:cg}} values that were attained by the real-world searchers, on average. We then performed a simple calculation for each interface and condition, dividing the average session time by the~\gls{acr:cg} values attained. This yielded the mean cost per unit of~\gls{acr:cg} (\genericblack{Cost/CG Unit}), a measure that demonstrates the \emph{efficiency} of searchers on average. The lowest the cost per unit of gain, the more efficient the searcher. From the table, we can clearly that the subjects employed for the study in Chapter~\ref{chap:diversity} searched in a more efficient way. The cost per unit of~\gls{acr:cg} was lower than that reported in Chapter~\ref{chap:snippets} (e.g. $155.29$ seconds over interface \blueboxbold{T2}, compared to $128.24$ seconds over condition \dualbluebox{ND}{AD}). Note the lower session times for the study reported in Chapter~\ref{chap:snippets} -- partially attributable to the $360$ second cutoff we imposed in that study.

- do the results for chapter 7, without the cutoff period imposed. Does the CG value rate higher? What is the rate per unit of gain without the cutoff? Does it get closer to the chapter 8 value?

- if so, shows that the strategies are bad overall at predicting performance.

While confirming the reasoning behind the 


best approximations for interface T2.
lowest MSE. why is that? probability estimates seemed to do a very good job for that particular interface.

interesting though is differences between Ch7 and Ch8.
why did the stopping strategies offer better approximations for ch7 than ch8? indeed, there were some interfaces where the stopping strategies ourperformed the real-world searchers.

ch8 searchers more efficient than ch7 searchers, hypothesise that the goals varies and influences your decisions/ability to search.

- mini table — cost per unit of gain.
		- could be a fact of the goal. When we look at the stopping strategies, in figure 8.9, searchers seem to be so much better than the stopping strategies!
		- unless the mean is dominated by one or two searchers who got one or good scores?

\subsection{Simulations of Interaction}

- why was the cg so poor for the simulations in chapter 8 compared to chapter 7?

- talk about the power of the experiments. were trends that we should of seen not visible?


% \subsection{Stopping Strategies and Behaviours}
%
% - simple strategies always perform best.
% - the more complex, the worse the performance and approximations get.
%     - this could be because of the way we instantiate.
%     - IFT/difference.
%         - you would expect perhaps with difference, the only strategies that look at the terms, that improvements would be observable as snippet lengths increase. however, this is not the case.
%
% - why does the IFT-based stopping heuristic do so badly? it's the rate of gain computation. our assumptions don't really work over a per-topic basis. think about it this way. consider a topic with a small number of rel documents, and a topic with a large number of rel docs. doesn't matter if it's a good query, it'll be harder to find relevant material for the more difficult topic! so the rate of gain we used doesn't work. so this will be interesting to explore in future work.
%
% - why are the different rules so bad? However, \blueboxbold{QS13} hides the bimodal distribution of performance that would have been experienced during searching. As we examine later, we posit that this may have been detrimental to difference-based stopping strategies \blueboxbold{SS6-DT} and \blueboxbold{SS6-DKL}. This is due to the face that for single term queries, the diversity of the results was likely to be much greater than for three term queries. Consequently, for single term queries that provide little gain, the simulated searcher was likely to have examined to much greater depths.
%
% for frustration rules, as i am more tolerant to nonrel information, i go deeper, has n effect of increased CG.
% satisfaction, if i look for one, then stop, tends to be better than going deeper. but kind of surprising.
% ift is pretty smooth, following the rate of gain is not the best. maybe it is how we calculate the rate of gain.
%
% - why does RBP underestimate? as you increase the patience parameter, you go deeper down the results. however, as you go deeper, you obtain better performance, but at a diminishing rate. like the difference based strategies, rbp is statistically signficant from ss11. except for T2, where no differences exist. rolling a dice doesnt perform as well. says it has a good user model, but we are showing maybe it doesn't.
%
% - pagination.
%
% - combination strategies do well!
%     - people consider how annoyed/satisfied they are. not a single criterion for judging when to stop.
%     -
%
%     \begin{table}[t!]
%         \caption[GAIN]{GAIN}
%         \label{tbl:conclusion_gain}
%         \renewcommand{\arraystretch}{1.8}
%         \begin{center}
%         \begin{tabulary}{\textwidth}{L{0.4cm}@{\CS}L{2.8cm}@{\CS}D{2.6cm}@{\CS}D{2.6cm}@{\CS}D{2.6cm}@{\CS}D{2.6cm}@{\CS}}
%
%             & & \lbluecell \textbf{Time/RS} & \lbluecell \textbf{Time/Doc.} & \lbluecell \textbf{CG} & \lbluecell \textbf{CG/Second} \\
%
%             \RS \multirow{4}{*}{\rotatebox{90}{\hspace*{-4mm}\textbf{Chapter~\ref{chap:snippets}}}} & \lbluecell\textbf{T0} & \cell xx & \cell xx & \cell xx & \cell xx \\
%             \RS & \lbluecell\textbf{T1} & \cell xx & \cell xx & \cell xx & \cell xx \\
%             \RS & \lbluecell\textbf{T2} & \cell xx & \cell xx & \cell xx & \cell xx \\
%             \RS & \lbluecell\textbf{T4} & \cell xx & \cell xx & \cell xx & \cell xx \\
%
%             \RS\RS\RS \multirow{4}{*}{\rotatebox{90}{\hspace*{-4mm}\textbf{Chapter~\ref{chap:diversity}}}} & \lbluecell\textbf{D-AS} & \cell xx & \cell xx & \cell xx & \cell xx \\
%             \RS & \lbluecell\textbf{ND-AS} & \cell xx & \cell xx & \cell xx & \cell xx \\
%             \RS & \lbluecell\textbf{D-AD} & \cell xx & \cell xx & \cell xx & \cell xx \\
%             \RS & \lbluecell\textbf{ND-AD} & \cell xx & \cell xx & \cell xx & \cell xx \\
%
%         \end{tabulary}
%         \end{center}
%     \end{table}
%
%
%
% \subsection{Improving Realism}
%
% - discuss the new decision point.
%
% - chapter 8 simulations...
% - however, in terms of performance, the simulated searchers don't do much better on D-AS than D-AD. why?
%     - probably because our decision maker didn't help much. doesn't differentiate between relevant and new and relevant.
%     - future work. how we instantiate the simulations.
%     - maybe this also accounts for why the stopping strategies (or some of them) appeared to underestimate the mean stopping depth. perhaps a better decision maker would be better for this.
%
% - does this account for the poor levels of mean cg that are attained in Figure 8.9 compared to the CG attained in Chapter 7?
%     - the real-world searchers performed generally better than all of the simulated stopping strategies.
%         - what does this suggest? is there another strategy out there that better fits this kind of task?
%         - only task that a stopping strategy did offer better approximations for in terms of CG is ND-AS. Which is T2. SS3 does better there. but why?
%
% \subsection{Simulations of Interaction}
%
% - then move onto a discussion of our simulation findings.
% - talk about the different issues that we raised.
%
% - moving to combination approaches, like bejewled~\cite{zhang2017bejewled}, etc.
%
% if a searcher performs badly/worse than always, they could be stopping queries early on that are bad, and entering those that are good, saving documents. likelihood that relevant documents already saved will reappear in subsequent queries later on, they have more time to issue more if they skip queries! and the judgement is such that if it's relevant, you go in. no consideration for things observed previously. did not consider this; so you enter a SERP, but there's nothing new to save even though it looked good! mismatch here -- something to address in future work.
%
% - talk about the simulations
%     - running the simulations. expensive. complicated.
%     - as we introduced the SERP level stopping decision point, the complexity increased massively -- and so did the time required to run them.
%     - talk about the pre-rolled judgements, too. methodological contribution. ensures repeatable, reproducible research.
%
% - did we do enough runs?
%     - we could justify why 50 DM runs was chosen.
%     - but was this enough? may not be.
%     - maybe we dont have enough trials.
%     - and when we dont have enough trials to average over, the power of our experiments could be insufficient.
%         - so we could be missing trends. we could be missing things that we simply cannot see.
%         - limitation placed upon me by the hardware available to run the experiments. future work.
%         - WE MENTION POWER IN SECTION~\ref{sec:methodology:user:flow} -- WITHIN-SUBJECTS INCREASES POWER. BUT WAS IT INCREASED ENOUGH?
%
% would have been better to do it over 50 topics, rather than five.
% previous papers show smoother curves.
% we tried to compensate for this by doing 50 runs.
% but probably not a good drop in solution.
% why did we do this though? not enough data. needed to do things in a consistent way.


\section{Conclusion}\label{sec:conclusions:conclusion}




\darkblueboxbold{HL-RQ1}


\darkblueboxbold{HL-RQ2}

\darkblueboxbold{HL-RQ3a}

\darkblueboxbold{HL-RQ3b}


- what is the conclusion?
    - there is no overall winning combination. simple strategies seem to do better in terms of offering outright performance, and approximations. in terms of approximations, tuning a given stopping strategy will yield good approximations, regardless of the strategy you follow. evident with SS1-FIX for example, with comparatively deep depths (i.e. x1=24).

- we also see improvements in modelling.
- at the expense of increased complexity (in terms of running them), the simulations have been shown to be more realistic in terms of approximating click depths.
    - it does appear that taking scent into account is important. leads to slightly lower error rates, although we could not demonstrate this difference to be significant.

- limitations of the work could influence work, hide trends.
- nevertheless, it is an important contribution that demonstrates how task and interface affect stopping behaviour.

\section{Future Research Directions}\label{sec:conclusions:future}
From the summary and discussion of our empirical results, a number of potential avenues for future work can be considered. In this penultimate section, we consider these possibilities, presenting them across four main categories. These are: how to improve the realism of simulations of interaction further; the consideration of stopping heuristics and strategies; considering simulation trials and topics; and considering the modelling of stopping from the level of individual searchers.

\subsection{Improving Simulation Realism}\label{sec:conclusions:future:improving}
In this thesis, we presented the~\gls{acr:csm}, a high-level, conceptual searcher model. It encapsulates many of the different activities and decision points that searchers would contend with across informational search tasks. With the inclusion of the new~\gls{acr:serp} level stopping decision point, improvements were made to the realism of the simulations that were executed with the~\gls{acr:csm}. However, \emph{what changes could we subsequently make to the~\gls{acr:csm} and related infrastructure in future work that would aid in advancing the realism of these simulations further?} As illustrated below, we consider this open question from three main research strands.

\begin{figure}[h!]
    \centering
    \resizebox{1\hsize}{!}{
    \includegraphics[width=1\textwidth]{figures/ch10-future_simulations.pdf}}
\end{figure}

\blueboxheader{Contextual and Cognitive}
Our first strand considers \genericblack{contextual} and \genericblack{cognitive} factors. All experimentation in this thesis was conducted under the domain of news search, with subjects of the user studies asked to imagine that they were newspaper reporters, having being given a task to find documents that they thought were relevant to a particular topic. However, this scenario is very specific. If we performed studies with the same methodology, but under a different search context, would we find similar results? Arguably, behaviours will change -- general web search and a detailed examination of content under the context we employed will result in different outcomes. Different tasks can also be considered. Aspectual and ad-hoc tasks were considered as we believed they would offer the greatest difference in terms of stopping behaviours. Would other retrieval tasks offer even bigger differences in terms of searcher behaviours?

Other factors such as the location, device and other external pressures will also undoubtedly influence the outcome of the results obtained. Crowdsourced subjects whose behaviours are reported in this thesis conducted our experiments on a desktop or laptop computer. They were instructed to be in a comfortable, quiet location, free from major distractions. In reality however, individuals are less likely to search in such conditions. Perhaps time pressures would influence their behaviours -- a student under pressure to finish a draft of her paper will behave differently to one who is not. With the proliferation of mobile devices such as smartphones, searching on such devices must also be considered. A recent study by~\cite{ong2017scent_behaviour} demonstrated that search behaviours for example do differ between those using desktop computers and smartphones.

Much work remains on how we can understand and subsequently model the cognitive processes and factors that influence how individuals behave when searching. Individuals are unique; behaviours will undoubtedly differ from person to person. Within the modelling process, novel techniques can be applied that could possibly improve the realism of simulations. For example, within the \simiir~framework, the search context component tracks a list of queries issued, documents examined (and saved), along with other measures. Could this component be manipulated in such a way as to better mimic the behaviours of a human? Rather than maintaining a perfect list of everything that has been examined, a simulated searcher could be programmed to become `forgetful' in remembering what they have examined, with certain cues within a document reminding them that this is something that they may have previously examined.

\blueboxheader{Conceptual Modelling}
We next consider a number of further enhancements to the~\gls{acr:csm} that could improve the realism of simulations further. Examples in the illustration above consider four potential areas for future improvement. \genericblack{Tool switching}, as demonstrated by~\cite{thomas2014modelling_behaviour}, would be considered at the beginning of the search process. It would enable a searcher to determine what tool, or retrieval system, would be suited to help them satisfy their information need. This is opposed to the current~\gls{acr:csm} as presented in the thesis, that assumes a retrieval system has been selected \emph{a priori.} A study by~\cite{white2009tool_switching} has shown that predicting tool switching may be feasible. They reported that sufficiently consistent behaviours exhibited by searchers in relation to this phenomenon led to accurate predictions of tool switching events.

\genericblack{Results pagination} is also listed in the illustration above. Here, a simulated searcher will be presented with~\glsplural{acr:serp} that are split across a number of different pages, rather than examining a continuous ranked list of results. This would involve the notion of extracting additional grounding data from interaction logs, perhaps such as the likelihood of a searcher continuing to the next~\gls{acr:serp} page, given their interaction history. This would likely impact upon the realism of simulations, as a study by~\cite{jansen2005analysis} showed a sharp decrease in content examined after the first page of results.

Further examination of modelling stopping behaviours within the~\gls{acr:csm} is also considered; refer to Section~\ref{sec:conclusions:future:stopping} for further details.

\blueboxheader{Stochastic to Deterministic} Decisions as to the attractiveness of result summaries and the relevance of documents within our simulations of interaction are determined \emph{stochastically,} or by a roll of the dice. While a simplifying assumption that has been used in many other studies employing simulations of interaction, this is an unrealistic approach. If implemented correctly, a more \emph{deterministic} solution would offer more realistic simulations, where simulated searchers would be able to \emph{learn} as they traverse through content, improving their decision making abilities based upon the content observed, rather than the outcome of a die roll. Advancements in understanding the \emph{information triage} process would undoubtedly lead to improved realism. In addition, the inclusion of \emph{variable interaction costs} would also benefit simulations.\footnote{As discussed previously in this thesis, \emph{time-biased gain}~\citep{smucker2012tbg} is an example of such an approach.}

\subsection{Stopping Heuristics and Strategies}\label{sec:conclusions:future:stopping}
In this thesis, we considered a total of twelve different stopping strategies, operationalised from a total of eight different stopping heuristics (and the~\gls{acr:rbp}~\gls{acr:ir} evaluation measure). We showed how each of the different strategies perform over a number of different experimental interfaces and conditions. During the methodological design stage, it became apparent that the approaches taken for the operationalisation of the selected stopping strategies was just one of many. \emph{What if we implemented our stopping strategies in different ways? Why did we select these strategies?} Here, we consider these questions with insight into what might happen if they were to be explored further.

\blueboxbold{Stopping Decision Points}
Following on with the theme of improving the underlying~\gls{acr:csm}, additional stopping decision points could be included. These would provides searchers subscribing to the~\gls{acr:csm} with more flexibility regarding when they stop examining content. Additional stopping decision points could for example include one for tool switching. In this example, a searcher, after spending some period of time on one retrieval system, could decide to stop using it after some criteria are met. After this point has been reached, they will then switch to retrieval system. A further interesting research question would be whether the the result summary level stopping strategies trialled in this thesis would work at different stopping decision points. For example, at a session level, would these strategies make sense? Would using them at that decision point lead to a better matchup with real-world stopping behaviours?

\blueboxbold{Stopping Strategy Selection}
From here, we can also consider a further decision point that could be encoded within the~\gls{acr:csm}. Inspired from \blueboxbold{SS11-COMB}, consideration must be taken into deciding \emph{why} and \emph{when} a particular stopping strategy could be employed. As we demonstrated in Figure~\ref{fig:ss11_combo} on page~\pageref{fig:ss11_combo}, \blueboxbold{SS11-COMB} employs both the frustration and give-up time-based stopping heuristics -- but not at once. Rather, a decision is made pertaining to the quality of the presented~\gls{acr:serp} (much like the~\gls{acr:serp} level stopping decision point). The outcome of this decision then dictates what stopping strategy is employed for the remainder of the query. Further refinements to this approach could for example include additional stopping strategies and a wider range of conditions for employing them. Empirical evidence could be extracted from interaction logs to determine if, under certain circumstances, searchers would favour one approach over another.

\blueboxbold{Stopping Strategy Operationalisation}
An open question arising from the work in this thesis considers: \emph{how do you operationalise the stopping heuristics?} Clearly, from the outline of the twelve stopping strategies in Chapter~\ref{chap:strategies} on page~\pageref{chap:strategies} (and implementation methodology in Section~\ref{sec:method:simulation:grounding:stopping} on page~\pageref{sec:method:simulation:grounding:stopping}), there are a large number different ways in which the stopping strategies can be implemented. While we provided a means and justification for the approaches that we took in this thesis, we have reason to believe that some of the stopping strategies -- especially \blueboxbold{SS6-DT}, \blueboxbold{SS7-DKL} and \blueboxbold{SS8-IFT} performed poorly, perhaps because of our implementations. For example, the rate of gain for \blueboxbold{SS8-IFT} could have been computed on a per topic basis. Further work will be required in order to determine if different implementations would lead to performance improvements.

\blueboxbold{Considering Additional Stopping Heuristics}
Of course, the eight stopping heuristics that we considered in this thesis do not constitute the entirety of the heuristics defined in the literature. We selected these heuristics as they offered interesting differences between one another, were \emph{relatively} straightforward to implement, and would likely be discernible across complex informational search tasks. Unused heuristics such as the mental list stopping heuristic (considering different criteria that must be met, as outlined by~\cite{nickles1995judgment} and detailed in Section~\ref{sec:stopping_background:heuristics:mental} on page~\pageref{sec:stopping_background:heuristics:mental}) would have been much more challenging to operationalise and implement -- and even so, would such a heuristic be suitable for the task at hand? The ability for a searcher to create a series of bullet points about a topic would imply he or she has some sound idea of what they are looking for. The searcher's knowledge of a topic may be so limited that such a heuristic would be unsuitable. Linking back to contextual factors above, considering additional search contexts (perhaps with searchers of astute and limited knowledge of a topic) would be interesting to examine.

\blueboxbold{Towards Future~\gls{acr:ir} Measures}
Given the above, findings from this research provides motivation for further work considering the inclusion of stopping heuristics within the measures that are used within~\gls{acr:ir} research. For example, stopping strategy \blueboxbold{SS5-COMB} demonstrated good overall and performance considering a searcher's satisfaction and tolerance to non-relevant material. This too has been shown in the BPM~\citep{zhang2017bejewled}, leading to the development of an updated evaluation measure considering the costs and gains of searching~\citep{azzopardi2018cwl}.

\subsection{Simulation Trials and Topics}\label{sec:conclusions:future:running}
We also consider future work in terms of \emph{how} the simulations of interaction could be run. While 50 trials were selected because of the fact that approximately 50 subjects partook in each user study, there may be trends and significant differences that we simply did not observe because of a lack of power. This limitation was also imposed with an insufficient amount of processing power to complete the experiments in a reasonable timeframe. With more powerful computer hardware, scaling up the experiments with more trials would have become a more realistic prospect.

We also consider using five topics for our performance \emph{(what-if)} experiments to be a limiting factor. While the decision to use five topics was justified due to a lack of data (considering entities across the remaining 45 topics in Chapter~\ref{chap:diversity}) -- and to ensure that comparisons between interfaces and conditions were fair -- 50 topics would have been preferred (refer to Figure~\ref{fig:per_topic_differences}). If data were available for the remaining 45 topics, we could then trial additional performance runs, which may also lead to the observation of other trends and potential significant differences.

\subsection{Individual Searcher Stopping Behaviours}
Our final consideration for future work revolves around the notion of \emph{individual searcher stopping behaviours.} In this thesis, we considered searcher stopping behaviours, reported across $\approx50$ subjects, over each interface and condition that was trialled. This provided us with a rough approximation as to what strategies work best, with similar findings reported across interfaces and conditions. However, research has shown that individual searcher behaviours are different. If we considered individual searchers, what trends would we then observe? Could we for example perform a classification of searcher stopping behaviours? For example, such an approach was followed by~\cite{smucker2011user_strategies}, who devised a classification of searchers when examining documents -- with searchers being categorised into one of either \emph{fast and liberal} or \emph{slow and neutral}.

\section{Final Remarks}\label{sec:conclusions:remarks}
Modelling search is complex, with searchers able to perform a wide range of actions and make a number of different decisions. We have shown in this thesis that stopping behaviours are also difficult to examine and measure. Nevertheless, we motivate the fact that we need to consider stopping behaviours in the development of future evaluation measures and search interfaces, with this work suggesting what strategies will perform well.

Despite the inherently difficult task that understanding and modelling stopping behaviours represents, we believe that the potential benefits of further exploration in this area will undoubtedly aid the searchers and researchers of future retrieval systems. Now that this is all done, \emph{I} am off to the pub -- and having reached the end of this thesis, you should go, too!