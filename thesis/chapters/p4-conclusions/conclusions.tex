%!TEX TS-program = xelatex
%!TEX root = ../../maxwell2018thesis.tex

\chapter[Discussion, Conclusions and Future Work]{Discussion, Conclusions\\and Future Work}\label{chap:conclusions}
With all experimental work now reported, this final chapter highlights the work that has been reported, and provides an in-depth discussion of the experimental results. This then leads onto a discussion of potential avenues for future work, before final, concluding remarks.

Finish flag

\section{Thesis Summary}

summarise what we did in one or two paragraphs. perhaps a bullet point list highlighting the main contributions, which we elaborate on below.


\section{Discussion and Contributions}

\begin{table}[t!]
    \caption[Experimental interfaces and conditions]{Experimental interfaces and conditions trialled in this chapter, based upon the work reported in Chapters~\ref{chap:snippets} and~\ref{chap:diversity}. In total, eight different experimental interfaces and conditioned were trialled, considering different result summary lengths, systems and tasks.}
    \label{tbl:serp_conditions}
    \renewcommand{\arraystretch}{1.8}
    \begin{center}
    \begin{tabulary}{\textwidth}{L{0.4cm}@{\CS}L{3.2cm}@{\CS}D{3.51cm}@{\CS}D{3.51cm}@{\CS}D{3.51cm}@{\CS}}

        & & \lbluecell \textbf{Summary Length} & \lbluecell \textbf{System} & \lbluecell \textbf{Task} \\

        \RS \multirow{4}{*}{\rotatebox{90}{\hspace*{-4mm}\textbf{Chapter~\ref{chap:snippets}}}} & \lbluecell\textbf{T0} & \cell \small{Title only} & \cell \small{\blueboxbold{ND} (Non Div.)} & \cell \small{\darkblueboxbold{AD} (Ad-hoc)}\\
        \RS & \lbluecell\textbf{T1} & \cell \small{Title + 1 snippet} & \cell \small{\blueboxbold{ND} (Non Div.)} & \cell \small{\darkblueboxbold{AD} (Ad-hoc)}\\
        \RS & \lbluecell\textbf{T2} & \cell \small{Title + 2 snippets} & \cell \small{\blueboxbold{ND} (Non Div.)} & \cell \small{\darkblueboxbold{AD} (Ad-hoc)}\\
        \RS & \lbluecell\textbf{T4} & \cell \small{Title + 4 snippets} & \cell \small{\blueboxbold{ND} (Non Div.)} & \cell \small{\darkblueboxbold{AD} (Ad-hoc)}\\
        
        \RS\RS\RS \multirow{4}{*}{\rotatebox{90}{\hspace*{-4mm}\textbf{Chapter~\ref{chap:diversity}}}} & \lbluecell\textbf{D-AS} & \cell \small{Title + 2 snippets} & \cell \small{\blueboxbold{D} (Div.)} & \cell \small{\darkblueboxbold{AS} (Aspectual)}\\
        \RS & \lbluecell\textbf{ND-AS} & \cell \small{Title + 2 snippets} & \cell \small{\blueboxbold{ND} (Non Div.)} & \cell \small{\darkblueboxbold{AS} (Aspectual)}\\
        \RS & \lbluecell\textbf{D-AD} & \cell \small{Title + 2 snippets} & \cell \small{\blueboxbold{D} (Div.)} & \cell \small{\darkblueboxbold{AD} (Ad-hoc)}\\
        \RS & \lbluecell\textbf{ND-AD} & \cell \small{Title + 2 snippets} & \cell \small{\blueboxbold{ND} (Non Div.)} & \cell \small{\darkblueboxbold{AD} (Ad-hoc)}\\
        
    \end{tabulary}
    \end{center}
\end{table}

- a good fixed-depth strategy can still yield very good performance, comparable to that of approaches that should intuitively make more sense in terms of yielding higher CG. This confirms findings in publications (maxwell2015).
- on a similar footing, why does the time-based strategy SS9 that is fixed outperform the adaptive strategy SS10? Maybe the gap between finding relevant documents is different. we should have done it over attractive result summaries instead.

- why does the satiation based rule perform so well?

- why does the IFT-based stopping heuristic do so badly? it's the rate of gain computation. our assumptions don't really work over a per-topic basis. think about it this way. consider a topic with a small number of rel documents, and a topic with a large number of rel docs. doesn't matter if it's a good query, it'll be harder to find relevant material for the more difficult topic! so the rate of gain we used doesn't work. so this will be interesting to explore in future work.

- why are the different rules so bad? However, \blueboxbold{QS13} hides the bimodal distribution of performance that would have been experienced during searching. As we examine later, we posit that this may have been detrimental to difference-based stopping strategies \blueboxbold{SS6-DT} and \blueboxbold{SS6-DKL}. This is due to the face that for single term queries, the diversity of the results was likely to be much greater than for three term queries. Consequently, for single term queries that provide little gain, the simulated searcher was likely to have examined to much greater depths. 

for frustration rules, as i am more tolerant to nonrel information, i go deeper, has n effect of increased CG.
satisfaction, if i look for one, then stop, tends to be better than going deeper. but kind of surprising.
ift is pretty smooth, following the rate of gain is not the best. maybe it is how we calculate the rate of gain.

- why does RBP underestimate? as you increase the patience parameter, you go deeper down the results. however, as you go deeper, you obtain better performance, but at a diminishing rate. like the difference based strategies, rbp is statistically signficant from ss11. except for T2, where no differences exist. rolling a dice doesnt perform as well. says it has a good user model, but we are showing maybe it doesn't.

- why does T2 consistently offer better predictions to actual searcher behaviour? Why are T2's MSE plots always lower than those of the other interfaces?
    - and why do none of the strategies for T2 offer better levels of CG? Maybe this is because the stopping strategy gives better approximations, and thus the CG levels wont be as high? there's a bit of a jump between the best SS and the RW.

- why was the chapter 8 simulation comparisons so bad in terms of CG attained, and number of documents saved with new entities? was it the decision maker that we decided to go with? was it the lack of a terminating condition?

- higher levels of gain were observed for tasks using system D when compared to system ND. This might be a consequence of the way in which we bubbled up TREC relevant documents only that were diverse.
    - but either way, system D should be doing this -- it should be bringing new aspects to the fore. again, confirms that the algorithm was working as expected.
    - and system D is able to get better gain at lower depths per query on average than system ND.

- however, in terms of performance, the simulated searchers don't do much better on D-AS than D-AD. why?
    - probably because our decision maker didn't help much. doesn't differentiate between relevant and new and relevant.
    - future work. how we instantiate the simulations.
    - maybe this also accounts for why the stopping strategies (or some of them) appeared to underestimate the mean stopping depth. perhaps a better decision maker would be better for this. 

- does this account for the poor levels of mean cg that are attained in Figure 8.9 compared to the CG attained in Chapter 7?
    - the real-world searchers performed generally better than all of the simulated stopping strategies.
        - what does this suggest? is there another strategy out there that better fits this kind of task?
        - only task that a stopping strategy did offer better approximations for in terms of CG is ND-AS. Which is T2. SS3 does better there. but why?
        
- for chapter 8, why does D have SS1 and ND have SS10 as the best approximation?

- for performance in chapter 8, mean depth per query is pretty consistent across stopping strategies. why is this the case?

- what would happen if gain is acquired from the snippets, and not the documents?

- why did SS5 do better in Chapter 8, but SS11 do better in Chapter 11 in terms of overall performance? Indeed, SS5 in chapter 8 offers consistently better approximations, too.

- little statistical difference in terms of chapter 8 performance. it doesn't really matter what strategy you use, you can still get good levels of CG.

- for chapter 8 comparisons, why was the CG much lower (on average) than the real world means?

- for D in CHapter 8, SS1 does the best approximations, but SS10 offers the best CG.

\subsection{Searcher Models}

how did we address \darkblueboxbold{HL-RQ1}? by introducing the CSM.
central to this work was the additional stopping decision point.
did this work?
empirically, we show that it did yield better approximations.
what can i steal from the ecir 2018 discussion?

\subsection{Examining Stopping Behaviours}

of course, the main focus of this thesis was stopping behaviours.
in particular, we took heuristics, operationalised them, and then tested them out.

see what happens to stopping behaviours 

\subsubsection{Stopping Heuristics}
we took a number of different heuristics and operationalised them!
however, this was not exhaustive.
and some heuristics that we did mention in the literature were not well suited to the search tasks we trialled.
for example, the single criterion rule was not operationalised.
different ways in which we could have implemented them, too! see future work.
this addressed \darkblueboxbold{HL-RQ2}.

\subsubsection{User Studies}
we examined two search contexts. Varying components. News search. Limitations! Does this generalise?

\subsubsection{Simulations of Interaction}
To address \darkblueboxbold{HL-RQ3a} and \darkblueboxbold{HL-RQ3b} we conducted simulations of interaction to determine the performance and approximations of each stopping strategy. 

Table -- what strategy did best under what contexts?
Was there a clear winner?




\section{Conclusion}

\section{Future Research Directions}
leading on from the findings of this thesis there are many different avenues of future work which can be explored.
we discuss a number of different directions 

\blueboxheader{Additional Stopping Decision Points}
Taking the CSM further. Tool switching. Other things to add to the searcher model to hitherto improve realism.

\blueboxheader{Additional Search Contexts}
Do the same findings apply to different search contexts? We considered only news search. And ad-hoc and aspectual retrieval. To see how things vary. But it is likely that they will vary elsewhere. Context, surroundings, device...

\blueboxheader{Additional Stopping Heuristics}

\blueboxheader{Heuristic Operationalisation}
We could have operationalised the stopping heuristics in many different ways. We chose simplistic means to operationalise them. These means may or may not have influenced the performance/approximations that each strategy offered. Future work is required to determine if this is indeed the case or not.

There are many, many different avenues for exploration here. Prime example: OFT strategy. Consistently performed poorly. List the different strategies that we could operationalise in a different way, explaining what may happen (educated guess).
For example, OFT, rate of gain set over all topics trialled. But we know that some topics are more difficult to find relevant material for than others (reference the table with the number of rel documents, show that one has less than the other). This would influence stopping, because simply less likely a relevant item would appear in the ranks.

\blueboxheader{Heuristic Selection}
We didn't really consider under what circumstances one heuristic would be better than another. So, how would one be able to select the best threshold for a given circumstance? Kind of eluded to this with SS11-COMB, that selected a stopping strategy based upon the perceived yield of a SERP. However, are there different factors which could influence the selection of a particular heuristic, and what are they? What heuristics would work best?

\blueboxheader{Individual Searcher Stopping}
How can we use the knowledge reported in this thesis to develop models more in line with an individual's stopping behaviours?
COMBO - recent work has shown that approaches like bejeweled are more in line with what people actually do. evidence from this thesis somewhat supports this argument -- combination approaches do well, and offer good performance. perhaps we need to look away from a single heuristic and consider two (or even more), see heuristic selection above.

Additionally, future work can work on these models at an individual searcher level. This thesis has exclusively reported behaviours on average, across \~ 50 subjects over different interfaces and conditions. This provides us with a rough approximation as to what will work best. However, people behave differently -- there is no general rule of thumb. For example, could we classify people based upon their searcher behaviours? Like Smucker's fast and liberal, slow and strict classification. Perhaps getting a better understanding of their searcher behaviours could factor into the interface used, and then this could improve their searching ability.

\blueboxheader{Improving Realism Further}
We made assumptions for the diversity study. The decision maker was based on ad-hoc retrieval.
The task for two was aspectual. This was not considered, yet we still got good gain and approximations. Would this improve if we undertook work to develop new decision making components, for the different retrieval tasks?


\section{Final Remarks}
a sentence of two explaining the key findings.



When considering possible future directions, Apple’s 1987 Knowledge Navigator vision of IR is still a strong exemplar of how search systems might develop. The short film showed a college professor pulling together a lecture presentation at the last minute. The professor used a form of tablet computer running an IR system presented as an agent capable of impeccable speech recognition, natural dialogue management, a high level of semantic understanding of the searcher’s information needs, as well as unbounded access to documents and federated databases.
The Knowledge Navigator identified and connected the professor to a colleague who helped him with the lecture. The broader implications of finding people (rather than documents) to aid with information needs that we see facilitated in the vast growth of social media was not really addressed in the Apple vision. What it also did not encompass was the portability of computer devices opening the possibility of serving information needs pertinent to the particular local context of location, location type, route, the company one is in, or a combination of all these factors. \todo{from the history of IR paper}



Limitations -- from the snippets paper, conclusions. We only focus on the news search. Surely people perform differently in different scenarios.